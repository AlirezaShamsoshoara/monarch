{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What is Monarch?\n",
        "Monarch is python-first language for describing distributed workloads. In this example, we use TorchTitan to demonstrate how Monarch can be used to run and orhestrate any workload.\n",
        "\n",
        "N.B. This notebook is under construction! Check back soon for examples including:\n",
        "1. Code synchronization APIs\n",
        "1. Remote breakpoints\n",
        "1. Improved Observability workloads\n",
        "\n",
        "\n",
        "## Step 1: Launch the job on Mast\n",
        "\n",
        "First, set up your local enviornment. Run the following commands to install TorchTitan/Monarch locally.\n",
        "\n",
        "1. Get your base conda env:\n",
        "\n",
        "    ```source /data/users/$USER/fbsource/genai/xlformers/dev/xl_conda.sh activate torchtitan_conda_prod:latest_conveyor_build```\n",
        "\n",
        "1. Install Titan:\n",
        "\n",
        "     `with-proxy pip install pyre-extensions torchao cloudpickle tyro datasets torchtitan`\n",
        "1. Install Monarch:\n",
        "\n",
        "    `with-proxy pip install --force-reinstall $(buck2 build @fbcode//mode/opt --show-full-simple-output 'fbcode//monarch/python/monarch:monarch_no_torch.whl')`\n",
        "1. Select your kernel from the dop down.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Steps to set up your conda env\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "from monarch.tools import commands\n",
        "from monarch.tools.components.meta import hyperactor\n",
        "from monarch.tools.config import Config\n",
        "from torchtitan.tools.logging import init_logger, logger\n",
        "from torchx.specs.fb.component_helpers import Packages\n",
        "\n",
        "\n",
        "def get_config(temp_dir) -> Config:\n",
        "    packages = Packages()\n",
        "    packages.add_package(\"oil.oilfs:stable\")\n",
        "    packages.add_package(\"manifold.manifoldfs\")\n",
        "\n",
        "    config = Config(\n",
        "        scheduler=\"mast_conda\",\n",
        "        scheduler_args={\n",
        "            # NOTE: replace with your own values\n",
        "            \"hpcIdentity\": \"pytorch_distributed\",\n",
        "            \"hpcJobOncall\": \"monarch\",\n",
        "            \"hpcClusterUuid\": \"MastProdCluster\",\n",
        "            \"rmAttribution\": \"pytorch4all_clients_approved\",\n",
        "        },\n",
        "        appdef=hyperactor.host_mesh_conda(\n",
        "            meshes=[\"mesh0:1:gtt_any\"],  # mesh_name:num_hosts:host_type\n",
        "            additional_packages=packages,\n",
        "        ),\n",
        "        workspace=temp_dir,\n",
        "    )\n",
        "    return config\n",
        "\n",
        "\n",
        "jobname: str = f\"monarch-{getpass.getuser()}\"\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "config = get_config(temp_dir)\n",
        "\n",
        "# NOTE: There's currently an issue with the MAST scheduler which sometimes causes \"pending\" jobs to fail at the following command.\n",
        "# If this happens, you can re-run the following command to connect to the job once it's running.\n",
        "await commands.get_or_create(jobname, config)\n",
        "os.rmdir(temp_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Launch Training\n",
        "\n",
        "Below, we wrap TorchTitanss train method in Monarch a Monarch Actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the BSD-style license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "import getpass\n",
        "import socket\n",
        "import subprocess\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from monarch._rust_bindings.monarch_hyperactor.alloc import AllocConstraints, AllocSpec\n",
        "from monarch._src.actor.actor_mesh import Actor, current_rank\n",
        "from monarch._src.actor.endpoint import endpoint\n",
        "from monarch._src.actor.meta.allocator import MastAllocator, MastAllocatorConfig\n",
        "from monarch._src.actor.proc_mesh import ProcMesh\n",
        "from monarch.meta.utils import setup_env_for_distributed\n",
        "\n",
        "from torchtitan.config_manager import ConfigManager, JobConfig\n",
        "\n",
        "from torchtitan.tools.logging import init_logger, logger\n",
        "from torchtitan.train import Trainer\n",
        "\n",
        "\n",
        "tokenizer_file_path = \"/tmp/monarch_titan_tokenizer_tokenizer.model\"\n",
        "mount_file_path = \"/tmp/mount_fs.sh\"\n",
        "\n",
        "\n",
        "def env_setup() -> None:\n",
        "    os.environ[\"NCCL_DEBUG\"] = \"INFO,WARN\"\n",
        "    os.environ[\"TORCH_SHOW_CPP_STACKTRACES\"] = \"1\"\n",
        "    os.environ[\"TORCH_ADDR2LINE_BINARY\"] = \"/packages/folly.symbolizer/folly-addr2line\"\n",
        "    os.environ[\"FUSE_DST\"] = \"/mnt/mffuse\"\n",
        "    os.environ[\"MANIFOLDFS_BUCKET\"] = \"torchtrain_datasets\"\n",
        "    # --- WS-Airstore configuration\n",
        "    os.environ[\"ENABLE_AIRSTORE\"] = \"\"\n",
        "    os.environ[\"DISABLE_OILFS\"] = \"1\"\n",
        "    os.environ[\"AIRSTORE_DECRYPT_SERVER_AFFINITY\"] = \"parent\"\n",
        "    os.environ[\"AIRSTORE_DECRYPT_SERVER_PATH\"] = (\n",
        "        \"/packages/ws_airstore.client/decrypt_server\"\n",
        "    )\n",
        "    os.environ[\"AIRSTORE_LOCAL_MOUNT_ROOT\"] = \"/mnt/airstore\"\n",
        "    # WS-AIRStore caches the shuffling, sharding information to enable fast startups\n",
        "    os.environ[\"AIRSTORE_INTERVAL_CACHE_DIR\"] = \"/mnt/airstore/airstore_metadata_cache\"\n",
        "    # For long running llamma4 production training jobs, please\n",
        "    #  set AIRSTORE_FBPKG_ID env var to ws_airstore.client:prod\n",
        "    os.environ[\"AIRSTORE_FBPKG_ID\"] = \"ws_airstore.client:prod\"\n",
        "    # --- OilFS\n",
        "    os.environ[\"WS_SSCV2_THRIFT_CONN_POOL_SIZE\"] = \"250000\"\n",
        "    # Only used for pretraining jobs. Perf tweaks for 8k+ gpu jobs\n",
        "    os.environ[\"OILFS_PROFILE\"] = \"pretraining\"\n",
        "\n",
        "\n",
        "async def create_mast_proc_mesh(\n",
        "    task_group: str,\n",
        "    job_name: str,\n",
        "    monarch_port: int = 26600,\n",
        "    num_hosts: int = 1,  # TODO: get the task_group size from MAST\n",
        "    num_gpus: int = 8,\n",
        ") -> ProcMesh:\n",
        "    \"\"\"Create a process mesh using MAST allocation.\n",
        "\n",
        "    Args:\n",
        "        task_group: The task group name\n",
        "        job_name: The MAST job name\n",
        "        monarch_port: The monarch port to use\n",
        "\n",
        "    Returns:\n",
        "        The created process mesh\n",
        "    \"\"\"\n",
        "    allocator = MastAllocator(\n",
        "        MastAllocatorConfig(\n",
        "            job_name=job_name,\n",
        "            remote_allocator_port=monarch_port,\n",
        "        ),\n",
        "    )\n",
        "    spec = AllocSpec(\n",
        "        AllocConstraints({MastAllocator.ALLOC_LABEL_TASK_GROUP: task_group}),\n",
        "        hosts=num_hosts,\n",
        "        gpus=num_gpus,\n",
        "    )\n",
        "    allocation = await allocator.allocate(spec)\n",
        "    hyperactor_mesh = await ProcMesh.from_alloc(allocation, env_setup)\n",
        "\n",
        "    return hyperactor_mesh\n",
        "\n",
        "\n",
        "def _get_hostname():\n",
        "    hostname = socket.gethostname()\n",
        "    return hostname\n",
        "\n",
        "\n",
        "class TrainerActorWrapper(Actor):\n",
        "    def __init__(\n",
        "        self, job_config: JobConfig, tokenizer_content: str, mount: str, env_to_merge={}\n",
        "    ):\n",
        "        self.job_config = job_config\n",
        "        self.rank = current_rank().rank\n",
        "        hostname = socket.gethostname()\n",
        "        print(f\"Initializing actor: {self.rank} {hostname=} {current_rank()=}\")\n",
        "        os.environ.update(env_to_merge)\n",
        "\n",
        "        if self.rank % 8 == 0:\n",
        "            # just use one actor to do the work\n",
        "            print(\n",
        "                f\"writing tokenizer to {tokenizer_file_path}; content size: {len(tokenizer_content)}\"\n",
        "            )\n",
        "            with open(tokenizer_file_path, \"w\") as tmp_file:\n",
        "                tmp_file.write(tokenizer_content)\n",
        "            print(f\"writing mount.sh to {mount_file_path}; content size: {len(mount)}\")\n",
        "            with open(mount_file_path, \"w\") as tmp_file:\n",
        "                tmp_file.write(mount)\n",
        "            os.chmod(mount_file_path, 0o777)\n",
        "            subprocess.run(\n",
        "                [mount_file_path], capture_output=True, text=True, check=True\n",
        "            )\n",
        "\n",
        "    @endpoint\n",
        "    def get_hostname(self):\n",
        "        return _get_hostname()\n",
        "\n",
        "    @endpoint\n",
        "    def train(self):\n",
        "        print(\"Starting training\")\n",
        "        logger.info(\"magicword: INFO: Starting training\")\n",
        "        logger.error(\"magicword: ERROR: Starting training\")\n",
        "        logger.critical(\"magicword: CRITICAL: Starting training\")\n",
        "        config = self.job_config\n",
        "        trainer: Optional[Trainer] = None\n",
        "\n",
        "        try:\n",
        "            trainer = Trainer(config)\n",
        "            trainer.train()\n",
        "\n",
        "            if config.checkpoint.create_seed_checkpoint:\n",
        "                assert (\n",
        "                    int(os.environ[\"WORLD_SIZE\"]) == 1\n",
        "                ), \"Must create seed checkpoint using a single device, to disable sharding.\"\n",
        "                assert (\n",
        "                    config.checkpoint.enable_checkpoint\n",
        "                ), \"Must enable checkpointing when creating a seed checkpoint.\"\n",
        "                trainer.checkpointer.save(curr_step=0, force=True)\n",
        "                logger.info(\"Created seed checkpoint\")\n",
        "            else:\n",
        "                trainer.train()\n",
        "        finally:\n",
        "            if trainer:\n",
        "                trainer.close()\n",
        "\n",
        "            if torch.distributed.is_initialized():\n",
        "                torch.distributed.destroy_process_group()\n",
        "                logger.info(\"Process group destroyed.\")\n",
        "        print(\"Done training\")\n",
        "\n",
        "\n",
        "async def async_main(job_config: JobConfig):\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "    job_name = f\"monarch-{getpass.getuser()}\"\n",
        "\n",
        "    print(f\"Spawning meshes on {job_name}\")\n",
        "    mast_proc_mesh = await create_mast_proc_mesh(\n",
        "        task_group=\"mesh0\",  # default TG for how we launched the job\n",
        "        job_name=job_name,\n",
        "        num_hosts=1,\n",
        "        num_gpus=8,  # this really only controls the number of workers each actor spawns\n",
        "    )\n",
        "    await setup_env_for_distributed(mast_proc_mesh)\n",
        "\n",
        "    await mast_proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)\n",
        "\n",
        "    # Read and ship tokenizer file to remote\n",
        "    file_path = os.path.expanduser(\n",
        "        f\"/home/{getpass.getuser()}/local/fbsource/fbcode/monarch/meta/torchtitan/workspace/test_tiktoken_tokenizer.model\"\n",
        "    )\n",
        "\n",
        "    mount_path = os.path.expanduser(\n",
        "        f\"/home/{getpass.getuser()}/local/fbsource/fbcode/ai_codesign/oss_infra_launch/torchtitan/mount.sh\"\n",
        "    )\n",
        "\n",
        "    with open(file_path, \"r\") as file:\n",
        "        content = file.read()\n",
        "    with open(mount_path, \"r\") as file:\n",
        "        mount = file.read()\n",
        "\n",
        "    print(job_config)\n",
        "    print(f\"Spawning meshes on {job_name}\")\n",
        "    trainer_actor = await mast_proc_mesh.spawn(\n",
        "        \"trainer_actor\", TrainerActorWrapper, job_config, content, mount, {}\n",
        "    )\n",
        "    await trainer_actor.train.call()\n",
        "\n",
        "    # print(f\"trainer_actor: {trainer_actor=}\")\n",
        "    # print(\"trainer_actor: waiting for rank0_hostname\")\n",
        "    # hostnames = await trainer_actor.get_hostname.call()\n",
        "    # print(\"got all hostnames\")\n",
        "    # rank0_hostname = hostnames.item(hosts=0, gpus=0)\n",
        "    # print(f\"{rank0_hostname=}\", flush=True)\n",
        "    # await trainer_actor.train.call(rank0_hostname)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    init_logger()\n",
        "    config_manager = ConfigManager()\n",
        "\n",
        "    mast_job_name = f\"monarch-{getpass.getuser()}\"\n",
        "    os.environ[\"MAST_HPC_JOB_NAME\"] = mast_job_name\n",
        "    manual_args = [\n",
        "        \"--job.config_file\",\n",
        "        os.path.expanduser(\"~/fbsource/fbcode/monarch/meta/torchtitan/llama3_8b.toml\"),\n",
        "        \"--model.tokenizer-path\",\n",
        "        tokenizer_file_path,\n",
        "        \"--training.steps\",\n",
        "        \"5\",\n",
        "        \"--training.dataset_path\",\n",
        "        \"/mnt/mffuse/c4\",\n",
        "        \"--job.dump_folder\",\n",
        "        \"/mnt/mffuse/outputs/\" + mast_job_name,\n",
        "    ]\n",
        "    config = config_manager.parse_args(manual_args)\n",
        "    await async_main(config)\n",
        "    print(\"All Done\")"
      ]
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "bebf729b-7176-4a45-bccd-6b7436080c15",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "monarch_env (local)",
      "language": "python",
      "name": "monarch_env_local"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
