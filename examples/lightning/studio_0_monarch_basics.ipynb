{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studio 0: Monarch Basics - Ping Pong Tutorial\n",
    "\n",
    "Welcome to the Lightning Studios Monarch series! This is **Studio 0**, where you'll learn the fundamentals of Monarch's Actor API through simple, hands-on examples.\n",
    "\n",
    "## What is Monarch?\n",
    "\n",
    "**Monarch** is Meta's distributed actor framework for building scalable, distributed applications. It makes it easy to:\n",
    "- Run code across multiple processes or machines\n",
    "- Coordinate distributed computations\n",
    "- Build complex distributed systems with simple Python code\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this tutorial, you'll learn:\n",
    "1. **Core Concepts**: Actors, Endpoints, and Process Meshes\n",
    "2. **Hello World**: Creating and calling actors\n",
    "3. **Calling Patterns**: Broadcasting vs. targeting specific actors\n",
    "4. **Actor Communication**: How actors talk to each other (Ping Pong!)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- Understanding of `async`/`await` (we'll provide a quick refresher)\n",
    "- Monarch installed (see [installation guide](https://github.com/meta-pytorch/monarch))\n",
    "\n",
    "## Lightning Studios Learning Path\n",
    "\n",
    "This is the **foundation** studio. After completing this, you can progress to:\n",
    "\n",
    "- **[Studio 1: Getting Started](./studio_1_getting_started.ipynb)** - Multi-node training with Lightning\n",
    "- **[Studio 2: Workspace Sync](./studio_2_workspace_sync.ipynb)** - Hot-reload configs without restarting\n",
    "- **[Studio 3: Interactive Debugging](./studio_3_interactive_debugging.ipynb)** - Debug distributed systems\n",
    "\n",
    "Let's dive in! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Core Concepts\n",
    "\n",
    "Before we write code, let's understand the key concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an Actor?\n",
    "\n",
    "Think of an **Actor** as an independent worker that:\n",
    "- Has its own state (variables)\n",
    "- Runs in its own process (possibly on a different machine)\n",
    "- Exposes **endpoints** (methods) that can be called remotely\n",
    "\n",
    "```\n",
    "┌─────────────────┐\n",
    "│  Actor Instance │\n",
    "│                 │\n",
    "│  State:         │\n",
    "│  - rank: 0      │\n",
    "│  - data: [...]  │\n",
    "│                 │\n",
    "│  Endpoints:     │\n",
    "│  - hello()      │\n",
    "│  - process()    │\n",
    "└─────────────────┘\n",
    "```\n",
    "\n",
    "## What is an Endpoint?\n",
    "\n",
    "An **Endpoint** is a method on an Actor that can be called remotely. It's marked with the `@endpoint` decorator.\n",
    "\n",
    "```python\n",
    "class MyActor(Actor):\n",
    "    @endpoint\n",
    "    async def my_method(self, arg):\n",
    "        # This can be called remotely!\n",
    "        return f\"Processed {arg}\"\n",
    "```\n",
    "\n",
    "## What is a Process Mesh?\n",
    "\n",
    "A **Process Mesh** (or ProcMesh) is a collection of processes where actors can be spawned. Think of it as a cluster of workers.\n",
    "\n",
    "```\n",
    "Process Mesh (4 GPUs)\n",
    "┌────────┬────────┬────────┬────────┐\n",
    "│ GPU 0  │ GPU 1  │ GPU 2  │ GPU 3  │\n",
    "│        │        │        │        │\n",
    "│ Actor  │ Actor  │ Actor  │ Actor  │\n",
    "│ Rank 0 │ Rank 1 │ Rank 2 │ Rank 3 │\n",
    "└────────┴────────┴────────┴────────┘\n",
    "```\n",
    "\n",
    "## Async/Await Quick Refresher\n",
    "\n",
    "Monarch uses Python's `async`/`await` for non-blocking operations:\n",
    "\n",
    "```python\n",
    "# Calling an endpoint\n",
    "result = await actor.my_method.call(\"hello\")  # Wait for result\n",
    "\n",
    "# Running multiple operations in parallel\n",
    "results = await asyncio.gather(\n",
    "    actor.method_1.call(),\n",
    "    actor.method_2.call(),\n",
    ")  # Wait for both to complete\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Hello World\n",
    "\n",
    "Let's create our first Monarch actor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Monarch\n",
    "\n",
    "First, import the necessary components from Monarch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from monarch.actor import Actor, current_rank, endpoint, proc_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Simple Actor\n",
    "\n",
    "Let's create a `ToyActor` that:\n",
    "- Stores its rank (unique ID)\n",
    "- Has a `hello_world` endpoint that prints a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTORS = 4\n",
    "\n",
    "\n",
    "class ToyActor(Actor):\n",
    "    def __init__(self):\n",
    "        # Get the rank (unique ID) of this actor instance\n",
    "        self.rank = current_rank().rank\n",
    "\n",
    "    @endpoint\n",
    "    async def hello_world(self, msg):\n",
    "        \"\"\"A simple endpoint that prints a message.\"\"\"\n",
    "        print(f\"Actor {self.rank}: Received message '{msg}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points\n",
    "\n",
    "- `Actor` base class: All Monarch actors inherit from this\n",
    "- `current_rank()`: Returns information about this actor's position in the mesh\n",
    "- `@endpoint`: Decorator that makes a method remotely callable\n",
    "- `async def`: Endpoints are async functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Process Mesh and Spawn Actors\n",
    "\n",
    "Now we'll:\n",
    "1. Create a process mesh with 4 processes\n",
    "2. Spawn 4 instances of `ToyActor` (one per process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_toy_actors():\n",
    "    # Create a local process mesh with 4 GPU slots\n",
    "    # Note: This works even without actual GPUs!\n",
    "    local_proc_mesh = proc_mesh(gpus=NUM_ACTORS)\n",
    "    \n",
    "    # Spawn 4 instances of ToyActor (one per GPU slot)\n",
    "    # This returns a \"handle\" to all instances\n",
    "    toy_actor = local_proc_mesh.spawn(\"toy_actor\", ToyActor)\n",
    "    \n",
    "    print(f\"✓ Spawned {NUM_ACTORS} ToyActor instances\")\n",
    "    \n",
    "    return toy_actor, local_proc_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding `proc_mesh(gpus=4)`\n",
    "\n",
    "This creates 4 processes. The parameter is called `gpus` because Monarch is often used for GPU computing, but it works fine without GPUs - it just means \"4 parallel processes.\"\n",
    "\n",
    "### Understanding `spawn()`\n",
    "\n",
    "When we call `spawn(\"toy_actor\", ToyActor)`:\n",
    "- Monarch creates 4 instances of `ToyActor`\n",
    "- Each runs in its own process\n",
    "- Each gets a unique rank (0, 1, 2, 3)\n",
    "- We get back a handle to communicate with all of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call All Actors at Once\n",
    "\n",
    "The most common pattern: broadcast a call to **all** actor instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192879/1306557437.py:4: DeprecationWarning: DEPRECATION WARNING: this function will soon be unsupported. Use this_host().spawn_procs(per_host = {'hosts': 2, 'gpus': 3}) instead of monarch.actor.proc_mesh(hosts=2, gpus=3).\n",
      "  local_proc_mesh = proc_mesh(gpus=NUM_ACTORS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spawned 4 ToyActor instances\n",
      "Actor 1: Received message 'Hello from main!'\n",
      "Actor 0: Received message 'Hello from main!'\n",
      "Actor 2: Received message 'Hello from main!'\n",
      "Actor 3: Received message 'Hello from main!'\n"
     ]
    }
   ],
   "source": [
    "async def call_all_actors():\n",
    "    toy_actor, local_proc_mesh = await create_toy_actors()\n",
    "    \n",
    "    # Call hello_world on ALL actor instances\n",
    "    # .call() broadcasts to all instances\n",
    "    await toy_actor.hello_world.call(\"Hello from main!\")\n",
    "    \n",
    "    return toy_actor, local_proc_mesh\n",
    "\n",
    "# Run it!\n",
    "toy_actor, toy_mesh = await call_all_actors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "You should see output from all 4 actors:\n",
    "```\n",
    "Actor 0: Received message 'Hello from main!'\n",
    "Actor 1: Received message 'Hello from main!'\n",
    "Actor 2: Received message 'Hello from main!'\n",
    "Actor 3: Received message 'Hello from main!'\n",
    "```\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "```\n",
    "       Main Process\n",
    "            │\n",
    "            ├──> toy_actor.hello_world.call(\"Hello\")\n",
    "            │\n",
    "    ┌───────┼───────┬───────┬───────┐\n",
    "    ▼       ▼       ▼       ▼       ▼\n",
    " Actor0  Actor1  Actor2  Actor3\n",
    "  Rank0   Rank1   Rank2   Rank3\n",
    "  print   print   print   print\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Calling Specific Actors\n",
    "\n",
    "Sometimes you want to call **specific** actor instances, not all of them. This is where `.slice()` comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Slice API\n",
    "\n",
    "`.slice()` lets you select specific actor instances:\n",
    "\n",
    "```python\n",
    "# Select actor at GPU 0\n",
    "actor_0 = toy_actor.slice(gpus=0)\n",
    "\n",
    "# Select actor at GPU 2\n",
    "actor_2 = toy_actor.slice(gpus=2)\n",
    "\n",
    "# Then call with .call_one()\n",
    "await actor_0.hello_world.call_one(\"Hi from actor 0!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Call Each Actor with a Unique Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor 1: Received message 'Unique message for actor 1'\n",
      "Actor 0: Received message 'Unique message for actor 0'\n",
      "Actor 2: Received message 'Unique message for actor 2'\n",
      "Actor 3: Received message 'Unique message for actor 3'\n",
      "\n",
      "✓ All specific actor calls completed\n"
     ]
    }
   ],
   "source": [
    "async def call_specific_actors():\n",
    "    futures = []\n",
    "    \n",
    "    for idx in range(NUM_ACTORS):\n",
    "        # Select the actor at index 'idx'\n",
    "        actor_instance = toy_actor.slice(gpus=idx)\n",
    "        \n",
    "        # Call with a unique message for this actor\n",
    "        future = actor_instance.hello_world.call_one(\n",
    "            f\"Unique message for actor {idx}\"\n",
    "        )\n",
    "        futures.append(future)\n",
    "    \n",
    "    # Wait for all calls to complete (in parallel!)\n",
    "    await asyncio.gather(*futures)\n",
    "    \n",
    "    print(\"\\n✓ All specific actor calls completed\")\n",
    "\n",
    "# Run it!\n",
    "await call_specific_actors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "```\n",
    "Actor 0: Received message 'Unique message for actor 0'\n",
    "Actor 1: Received message 'Unique message for actor 1'\n",
    "Actor 2: Received message 'Unique message for actor 2'\n",
    "Actor 3: Received message 'Unique message for actor 3'\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "We used `asyncio.gather()` to schedule all calls in parallel. Without `gather()`, they'd run sequentially (slower).\n",
    "\n",
    "```\n",
    "Sequential (slow):        Parallel with gather() (fast):\n",
    "┌────┐                    ┌────┐\n",
    "│ A0 │────┐               │ A0 │────┐\n",
    "└────┘    │               ├────┤    │\n",
    "          │               │ A1 │────┤\n",
    "┌────┐    │               ├────┤    ├─> All complete!\n",
    "│ A1 │────┤               │ A2 │────┤\n",
    "└────┘    │               ├────┤    │\n",
    "          │               │ A3 │────┘\n",
    "┌────┐    │               └────┘\n",
    "│ A2 │────┤\n",
    "└────┘    │\n",
    "          │\n",
    "┌────┐    │\n",
    "│ A3 │────┘\n",
    "└────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: `.call()` vs `.call_one()`\n",
    "\n",
    "| Method | Use Case | Example |\n",
    "|--------|----------|----------|\n",
    "| `.call()` | Broadcast to **all** instances | `actor.method.call(arg)` |\n",
    "| `.call_one()` | Call a **specific** instance (after `.slice()`) | `actor.slice(gpus=0).method.call_one(arg)` |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "- **`.call()`**: When you want all actors to do the same thing\n",
    "  - Example: Initialize all actors, broadcast data, synchronize state\n",
    "  \n",
    "- **`.call_one()` with `.slice()`**: When you want specific behavior per actor\n",
    "  - Example: Assign different data partitions, target specific workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Actor-to-Actor Communication (Ping Pong!)\n",
    "\n",
    "So far, we've called actors from our main code. But actors can also **talk to each other**! This is powerful for building distributed systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Ping Pong Example\n",
    "\n",
    "We'll create two groups of actors that send messages to each other:\n",
    "\n",
    "```\n",
    "Actor Group 0              Actor Group 1\n",
    "┌──────────┐               ┌──────────┐\n",
    "│ Actor 0  │──── Ping ───> │ Actor 0  │\n",
    "│ Actor 1  │               │ Actor 1  │\n",
    "└──────────┘               └──────────┘\n",
    "                              │\n",
    "                            Pong!\n",
    "                              │\n",
    "┌──────────┐               ┌──────────┐\n",
    "│ Actor 0  │ <─── Ping ─── │ Actor 0  │\n",
    "│ Actor 1  │               │ Actor 1  │\n",
    "└──────────┘               └──────────┘\n",
    "   │\n",
    " Pong!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the PingPong Actor\n",
    "\n",
    "This actor can:\n",
    "- Store a reference to another actor\n",
    "- Send messages to that actor\n",
    "- Receive messages from that actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PingPongActor(Actor):\n",
    "    def __init__(self, actor_name):\n",
    "        \"\"\"Initialize with a name to identify this actor group.\"\"\"\n",
    "        self.actor_name = actor_name\n",
    "        self.identity = None\n",
    "        self.other_actor = None\n",
    "        self.other_actor_pair = None\n",
    "\n",
    "    @endpoint\n",
    "    async def init(self, other_actor):\n",
    "        \"\"\"\n",
    "        Initialize this actor with a reference to another actor.\n",
    "        \n",
    "        Key insight: We store a 'slice' of the other actor that corresponds\n",
    "        to our rank. So Actor 0 will talk to the other Actor 0, \n",
    "        Actor 1 to the other Actor 1, etc.\n",
    "        \"\"\"\n",
    "        self.other_actor = other_actor\n",
    "        \n",
    "        # Get my rank\n",
    "        self.identity = current_rank().rank\n",
    "        \n",
    "        # Slice the other actor to get my \"pair\" (same rank)\n",
    "        self.other_actor_pair = other_actor.slice(**current_rank())\n",
    "        \n",
    "        print(f\"[{self.actor_name}:{self.identity}] Initialized and paired with other actor\")\n",
    "\n",
    "    @endpoint\n",
    "    async def send(self, msg):\n",
    "        \"\"\"Send a message to our paired actor in the other group.\"\"\"\n",
    "        await self.other_actor_pair.recv.call(\n",
    "            f\"Sender ({self.actor_name}:{self.identity}) says: {msg}\"\n",
    "        )\n",
    "\n",
    "    @endpoint\n",
    "    async def recv(self, msg):\n",
    "        \"\"\"Receive a message from our paired actor.\"\"\"\n",
    "        print(f\"Pong! [{self.actor_name}:{self.identity}] received: {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Code\n",
    "\n",
    "**The `init` endpoint:**\n",
    "- Takes a reference to another actor group\n",
    "- Uses `.slice(**current_rank())` to pair actors by rank\n",
    "  - Actor 0 in group A pairs with Actor 0 in group B\n",
    "  - Actor 1 in group A pairs with Actor 1 in group B\n",
    "\n",
    "**The `send` endpoint:**\n",
    "- Calls `recv` on the paired actor\n",
    "- This demonstrates **actor-to-actor communication**!\n",
    "\n",
    "**The `recv` endpoint:**\n",
    "- Receives and prints the message\n",
    "- The \"Pong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Two Actor Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192879/3579550400.py:3: DeprecationWarning: DEPRECATION WARNING: this function will soon be unsupported. Use this_host().spawn_procs(per_host = {'hosts': 2, 'gpus': 3}) instead of monarch.actor.proc_mesh(hosts=2, gpus=3).\n",
      "  local_mesh_0 = proc_mesh(gpus=2)\n",
      "/tmp/ipykernel_192879/3579550400.py:11: DeprecationWarning: DEPRECATION WARNING: this function will soon be unsupported. Use this_host().spawn_procs(per_host = {'hosts': 2, 'gpus': 3}) instead of monarch.actor.proc_mesh(hosts=2, gpus=3).\n",
      "  local_mesh_1 = proc_mesh(gpus=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Created two actor groups (2 actors each)\n"
     ]
    }
   ],
   "source": [
    "async def create_ping_pong_actors():\n",
    "    # Create first mesh with 2 actors\n",
    "    local_mesh_0 = proc_mesh(gpus=2)\n",
    "    actor_0 = local_mesh_0.spawn(\n",
    "        \"actor_0\",\n",
    "        PingPongActor,\n",
    "        \"GroupA\",  # This argument is passed to __init__\n",
    "    )\n",
    "\n",
    "    # Create second mesh with 2 actors\n",
    "    local_mesh_1 = proc_mesh(gpus=2)\n",
    "    actor_1 = local_mesh_1.spawn(\n",
    "        \"actor_1\",\n",
    "        PingPongActor,\n",
    "        \"GroupB\",  # This argument is passed to __init__\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Created two actor groups (2 actors each)\")\n",
    "\n",
    "    return actor_0, actor_1, local_mesh_0, local_mesh_1\n",
    "\n",
    "# Create the actors\n",
    "actor_group_a, actor_group_b, mesh_a, mesh_b = await create_ping_pong_actors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Have Now\n",
    "\n",
    "```\n",
    "Group A (actor_group_a)        Group B (actor_group_b)\n",
    "┌──────────────────┐           ┌──────────────────┐\n",
    "│ GroupA Actor 0   │           │ GroupB Actor 0   │\n",
    "│ GroupA Actor 1   │           │ GroupB Actor 1   │\n",
    "└──────────────────┘           └──────────────────┘\n",
    "\n",
    "They don't know about each other yet!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize: Pair the Actors\n",
    "\n",
    "Now we'll tell each actor group about the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GroupA:0] Initialized and paired with other actor\n",
      "[GroupA:1] Initialized and paired with other actor\n",
      "[GroupB:0] Initialized and paired with other actor\n",
      "[GroupB:1] Initialized and paired with other actor\n",
      "\n",
      "✓ Actors are now paired and ready to communicate!\n"
     ]
    }
   ],
   "source": [
    "async def init_ping_pong(actor_0, actor_1):\n",
    "    # Initialize actors with references to each other\n",
    "    # We do this in parallel using asyncio.gather\n",
    "    await asyncio.gather(\n",
    "        actor_0.init.call(actor_1),  # Group A learns about Group B\n",
    "        actor_1.init.call(actor_0),  # Group B learns about Group A\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Actors are now paired and ready to communicate!\")\n",
    "\n",
    "# Initialize the pairing\n",
    "await init_ping_pong(actor_group_a, actor_group_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Initialization\n",
    "\n",
    "```\n",
    "Group A                         Group B\n",
    "┌──────────────────┐           ┌──────────────────┐\n",
    "│ GroupA Actor 0   │ <──────>  │ GroupB Actor 0   │\n",
    "│                  │   paired   │                  │\n",
    "│ GroupA Actor 1   │ <──────>  │ GroupB Actor 1   │\n",
    "└──────────────────┘   paired   └──────────────────┘\n",
    "\n",
    "Each actor knows its \"pair\" in the other group!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send Messages Between Actors\n",
    "\n",
    "Now for the exciting part - let's make them talk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Ping Pong Communication\n",
      "============================================================\n",
      "\n",
      "📤 Group A sending 'Ping!' to Group B...\n",
      "\n",
      "Pong! [GroupB:0] received: Sender (GroupA:0) says: Ping!\n",
      "Pong! [GroupB:1] received: Sender (GroupA:1) says: Ping!\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "📤 Group B sending 'Ping!' to Group A...\n",
      "\n",
      "Pong! [GroupA:0] received: Sender (GroupB:0) says: Ping!\n",
      "Pong! [GroupA:1] received: Sender (GroupB:1) says: Ping!\n",
      "\n",
      "============================================================\n",
      "✓ Ping Pong Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "async def send_ping_pong(actor_0, actor_1):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting Ping Pong Communication\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Group A sends \"Ping!\" to Group B\n",
    "    print(\"📤 Group A sending 'Ping!' to Group B...\\n\")\n",
    "    await actor_0.send.call(\"Ping!\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "    \n",
    "    # Group B sends \"Ping!\" to Group A\n",
    "    print(\"📤 Group B sending 'Ping!' to Group A...\\n\")\n",
    "    await actor_1.send.call(\"Ping!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ Ping Pong Complete!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run the ping pong!\n",
    "await send_ping_pong(actor_group_a, actor_group_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "```\n",
    "📤 Group A sending 'Ping!' to Group B...\n",
    "\n",
    "Pong! [GroupB:0] received: Sender (GroupA:0) says: Ping!\n",
    "Pong! [GroupB:1] received: Sender (GroupA:1) says: Ping!\n",
    "\n",
    "📤 Group B sending 'Ping!' to Group A...\n",
    "\n",
    "Pong! [GroupA:0] received: Sender (GroupB:0) says: Ping!\n",
    "Pong! [GroupA:1] received: Sender (GroupB:1) says: Ping!\n",
    "```\n",
    "\n",
    "### What Happened?\n",
    "\n",
    "1. **Group A's Actor 0** called `send(\"Ping!\")`\n",
    "2. This invoked `recv()` on **Group B's Actor 0** (its pair)\n",
    "3. Group B's Actor 0 printed \"Pong!\"\n",
    "4. Same for Actor 1 in both groups\n",
    "5. Then we reversed the direction!\n",
    "\n",
    "```\n",
    "     GroupA Actor 0  ──send()──>  GroupB Actor 0\n",
    "                                       │\n",
    "                                    recv()\n",
    "                                       │\n",
    "                                   Pong!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎉 Congratulations! 🎉\n",
    "\n",
    "You've learned the fundamentals of Monarch!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "### Core Concepts\n",
    "- ✓ **Actors**: Independent workers with state and endpoints\n",
    "- ✓ **Endpoints**: Remotely callable methods (with `@endpoint`)\n",
    "- ✓ **Process Mesh**: Collection of processes for spawning actors\n",
    "\n",
    "### Calling Patterns\n",
    "- ✓ **`.call()`**: Broadcast to all actor instances\n",
    "- ✓ **`.slice()`**: Select specific actor instances\n",
    "- ✓ **`.call_one()`**: Call a specific sliced actor\n",
    "\n",
    "### Communication\n",
    "- ✓ **Main → Actor**: Call endpoints from your code\n",
    "- ✓ **Actor → Actor**: Actors calling other actors' endpoints\n",
    "- ✓ **Pairing**: Using `.slice(**current_rank())` to pair actors\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Actors run independently** in separate processes\n",
    "2. **Endpoints are async** - use `await` when calling them\n",
    "3. **Use `.call()` for broadcast**, `.call_one()` for targeted calls\n",
    "4. **Actors can reference other actors** for complex distributed systems\n",
    "5. **`asyncio.gather()` runs operations in parallel** for better performance\n",
    "\n",
    "## Next Steps: Lightning Studios Series\n",
    "\n",
    "Now that you understand Monarch basics, continue your journey with the Lightning Studios:\n",
    "\n",
    "### 🚀 Studio 1: Getting Started (Recommended Next!)\n",
    "**[studio_1_getting_started.ipynb](./studio_1_getting_started.ipynb)**\n",
    "\n",
    "Learn how to run distributed multi-node training:\n",
    "- Launch multi-node jobs on Lightning AI\n",
    "- Set up distributed process meshes across machines\n",
    "- Run TorchTitan training for Llama-3-8B\n",
    "- Scale from 2 to 16+ nodes\n",
    "\n",
    "### 🔄 Studio 2: Workspace Synchronization\n",
    "**[studio_2_workspace_sync.ipynb](./studio_2_workspace_sync.ipynb)**\n",
    "\n",
    "Master hot-reloading for faster iteration:\n",
    "- Sync local code/config changes to remote nodes\n",
    "- Update training configs without restarting jobs\n",
    "- 10x faster iteration cycles\n",
    "\n",
    "### 🐛 Studio 3: Interactive Debugging\n",
    "**[studio_3_interactive_debugging.ipynb](./studio_3_interactive_debugging.ipynb)**\n",
    "\n",
    "Debug distributed systems like a pro:\n",
    "- Set breakpoints in distributed actors\n",
    "- Inspect environment variables across nodes\n",
    "- Use `monarch debug` CLI for interactive debugging\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### 📚 More Examples\n",
    "Check out these examples in the docs:\n",
    "- `getting_started.py` - More Monarch fundamentals\n",
    "- `distributed_tensors.py` - Working with tensors across actors\n",
    "- `debugging.py` - Debugging distributed actors\n",
    "- `spmd_ddp.py` - Distributed data parallel training\n",
    "\n",
    "### 📖 Documentation\n",
    "- [Monarch GitHub](https://github.com/meta-pytorch/monarch)\n",
    "- [Monarch Documentation](https://github.com/meta-pytorch/monarch/tree/main/docs)\n",
    "- [TorchTitan with Monarch](https://github.com/pytorch/torchtitan)\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "Here are some exercises to reinforce your learning:\n",
    "\n",
    "1. **Modify `ToyActor`** to return a value instead of printing\n",
    "2. **Create a chain** of 3 actor groups where A → B → C → A\n",
    "3. **Add a counter** to `PingPongActor` that tracks messages sent/received\n",
    "4. **Experiment with different mesh sizes** - try 8 or 16 actors\n",
    "\n",
    "Ready for real-world distributed training? Head to **[Studio 1](./studio_1_getting_started.ipynb)** next!\n",
    "\n",
    "Happy coding with Monarch! 🎊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Cleanup\n",
    "\n",
    "When you're done, it's good practice to stop the process meshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All process meshes stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[-]E1021 06:22:39.651440 192879 hyperactor/src/mailbox.rs:789] message not delivered, broken link: message was undeliverable, name:undelivered_message_abandoned, actor_name:client, actor_id:unix:@MDJ1jdlthP4sXksMQs3WfCYy,mesh_root_client_proc,client[0], dest:unix:@5gZ2uZib5A1X4l2DMSfUBBvD,anon_0_1CLnTo3eunAx,comm_1BrWrVYAYy4d[0][13147652568889606402<hyperactor_mesh::comm::multicast::CastMessage>], headers:hyperactor::mailbox::headers::send_timestamp=2025-10-21T06:22:09.569999074+00:00,hyperactor_mesh::actor_mesh::cast_actor_mesh_id=agent, data:CastMessage{\"dest\":{\"selection\":\"True\",\"slice\":{\"offset\":0,\"sizes\":[1,4],\"strides\":[4,1]}},\"message\":{\"actor_mesh_id\":{\"V1\":{\"Reserved\":\"agent\"}},\"data\":{\"bindings\":\"[[119644[...415 chars] CRC:9810abf9 5b 5b 31 31 39 36 34 34 [...407 bytes]\",\"message\":{\"encoded\":{\"Multipart\":{\"body\":\"[0,0,0,0[...379 chars] CRC:956b70a6 5b 30 2c 30 2c 30 2c 30 [...371 bytes]\",\"is_illegal\":false,\"parts\":[]}},\"typehash\":2686893944490142614}},\"dest_port\":{\"actor_name\":\"agent\",\"port\":16120262460791718278},\"sender\":\"[{\\\"Direc[...86 chars] CRC:3e99203d 5b 7b 22 44 69 72 65 63 [...78 bytes]\",\"shape\":{\"labels\":\"[\\\"hosts\\\"[...16 chars] CRC:c6cd3273 5b 22 68 6f 73 74 73 22 [...8 bytes]\",\"slice\":{\"offset\":0,\"sizes\":[1,4],\"strides\":[4,1]}}}}\n",
      "[0]E1021 06:22:39.648881 244291 hyperactor/src/channel/net.rs:885] error_msg:session unix:@ceseKritv6jJB5ZyeZTU3OSW.18033795279184453286: failed to receive ack within timeout 30 secs; link is currently broken\n",
      "[0]E1021 06:22:39.843078 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@XbIJCbb7RgKlt7RWLEoV3yr6.11413841067414787042: failed to deliver message within timeout\n",
      "[0]E1021 06:22:39.954391 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@VZY4Z9MkBZUSuom6cD3r5xHY.248166809154178596: failed to deliver message within timeout\n",
      "[0]E1021 06:22:39.977652 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@XbIJCbb7RgKlt7RWLEoV3yr6.6515750690386783878: failed to deliver message within timeout\n",
      "[0]E1021 06:22:39.999110 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@7vVpXC8nWAUeLXO0VGpVGQco.16975570356545722714: failed to deliver message within timeout\n",
      "[0]E1021 06:22:40.090407 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@Rm0r73XuR6XXsRFIMuLE4JgF.16772007913526825128: failed to deliver message within timeout\n",
      "[0]E1021 06:22:40.096770 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@7qOgGSxsC51fNkmA8amaGsue.10039746471530538760: failed to deliver message within timeout\n",
      "[0]E1021 06:22:40.097906 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@UeKMhaTJouJXlbhZzAc0LK2B.18362425238001301143: failed to deliver message within timeout\n",
      "[0]E1021 06:22:40.111075 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@9jQyKF10JxTeq4dppYGeqdYh.9133130741250403553: failed to deliver message within timeout\n",
      "[0]E1021 06:22:40.238403 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@VZY4Z9MkBZUSuom6cD3r5xHY.16862440995233208747: failed to deliver message within timeout\n",
      "[0]E1021 06:22:40.259595 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@9jQyKF10JxTeq4dppYGeqdYh.17051839896845107496: failed to deliver message within timeout\n",
      "[0]E1021 06:22:40.300904 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@UeKMhaTJouJXlbhZzAc0LK2B.8527202483917652592: failed to deliver message within timeout\n",
      "[0]E1021 06:22:40.318121 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@7qOgGSxsC51fNkmA8amaGsue.7676598537295674337: failed to deliver message within timeout\n",
      "[0]E1021 06:22:40.640357 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@Rm0r73XuR6XXsRFIMuLE4JgF.10664436456012452846: failed to deliver message within timeout\n",
      "[0]E1021 06:22:40.683625 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@7vVpXC8nWAUeLXO0VGpVGQco.11020115442775055072: failed to deliver message within timeout\n",
      "[0]E1021 06:22:40.690992 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@OBSnCzKQrl1i41ow0r4LAGAX.7901598446581669846: failed to deliver message within timeout\n",
      "[0]E1021 06:22:40.719190 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@OBSnCzKQrl1i41ow0r4LAGAX.4876119994074827561: failed to deliver message within timeout\n",
      "[0]E1021 06:23:10.142461 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@zNxgzF0BKhQQmnouf73mwRYX.13238124326749343182: failed to deliver message within timeout\n",
      "[0]E1021 06:23:10.142728 244291 hyperactor/src/mailbox.rs:789] message not delivered, broken link: message returned to undeliverable port, name:undelivered_message_abandoned, actor_name:comm_1BrWrVYAYy4d, actor_id:unix:@5gZ2uZib5A1X4l2DMSfUBBvD,anon_2_1eDVLnGViWMQ,comm_1BrWrVYAYy4d[0], dest:unix:@5gZ2uZib5A1X4l2DMSfUBBvD,anon_2_1eDVLnGViWMQ,comm_1BrWrVYAYy4d[0][13718074585354237591<unknown>], headers:, data:Encoded::Multipart(illegal?=false body=CRC:153b00ef 1 0 0 0 4 0 0 0 [...277 bytes], part[0]=CRC:db7b250d 0 0 0 0 9 0 0 0 [...148 bytes])\n",
      "[0]E1021 06:23:10.233321 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@491O46NIcEyVqi2Ee3JDa1mp.1414268966897363513: failed to deliver message within timeout\n",
      "[0]E1021 06:23:10.233455 244291 hyperactor/src/mailbox.rs:789] message not delivered, broken link: message returned to undeliverable port, name:undelivered_message_abandoned, actor_name:comm_1BrWrVYAYy4d, actor_id:unix:@5gZ2uZib5A1X4l2DMSfUBBvD,anon_3_1hYx6jrw3JTH,comm_1BrWrVYAYy4d[0], dest:unix:@5gZ2uZib5A1X4l2DMSfUBBvD,anon_3_1hYx6jrw3JTH,comm_1BrWrVYAYy4d[0][13718074585354237591<unknown>], headers:, data:Encoded::Multipart(illegal?=false body=CRC:2dad6b4d 1 0 0 0 4 0 0 0 [...277 bytes], part[0]=CRC:8df31578 0 0 0 0 9 0 0 0 [...148 bytes])\n",
      "[-]E1021 06:23:10.447021 192879 hyperactor/src/channel/net.rs:872] error_msg:session unix:@4d5X8F5iyjQWgFeCnGPhFOV2.13308820280474659213: failed to deliver message within timeout\n",
      "[0]E1021 06:23:10.538939 244291 hyperactor/src/channel/net.rs:872] error_msg:session unix:@DCzcxdX96e6gcL0zlFD69ogj.3599297564851604845: failed to deliver message within timeout\n",
      "[0]E1021 06:23:10.539046 244291 hyperactor/src/mailbox.rs:789] message not delivered, broken link: message returned to undeliverable port, name:undelivered_message_abandoned, actor_name:comm_1BrWrVYAYy4d, actor_id:unix:@5gZ2uZib5A1X4l2DMSfUBBvD,anon_1_1odf5oJUoyUJ,comm_1BrWrVYAYy4d[0], dest:unix:@5gZ2uZib5A1X4l2DMSfUBBvD,anon_1_1odf5oJUoyUJ,comm_1BrWrVYAYy4d[0][13718074585354237591<unknown>], headers:, data:Encoded::Multipart(illegal?=false body=CRC:ce3dfcb6 1 0 0 0 4 0 0 0 [...277 bytes], part[0]=CRC:198a64c2 0 0 0 0 9 0 0 0 [...148 bytes])\n",
      "[0]E1021 07:45:35.011804 244291 hyperactor/src/channel/net.rs:746] error_msg:session unix:@983zBx4MN5zriy3EZpojnJX9.16349855802834370687: failed to receive ack within timeout 30 secs; link is currently connected\n",
      "[0]E1021 07:45:35.012194 244291 hyperactor_mesh/src/bootstrap.rs:187] Heartbeat failed to allocator, exiting! (addr: Unix(Bound(\"983zBx4MN5zriy3EZpojnJX9\" (abstract)))): channel closed\n"
     ]
    }
   ],
   "source": [
    "# Stop the meshes\n",
    "await toy_mesh.stop()\n",
    "await mesh_a.stop()\n",
    "await mesh_b.stop()\n",
    "\n",
    "print(\"✓ All process meshes stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
