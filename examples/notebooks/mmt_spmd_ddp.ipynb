{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Machine Training (MMT) SPMD DDP Example\n",
        "\n",
        "This notebook demonstrates how to integrate the Lightning SDK's MMT API with Monarch's SPMD DDP training to enable distributed training across multiple nodes.\n",
        "\n",
        "## Overview\n",
        "\n",
        "- **MMT (Multi-Machine Training)**: Lightning SDK's API for distributed computing\n",
        "- **SPMD (Single Program, Multiple Data)**: Parallel computing approach\n",
        "- **DDP (Distributed Data Parallel)**: PyTorch's distributed training method\n",
        "\n",
        "This example shows how to scale PyTorch training across multiple machines using these technologies together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Dependencies\n",
        "\n",
        "First, let's import all the necessary libraries for multi-node distributed training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from lightning_sdk import Machine, MMT, Studio\n",
        "from monarch.actor import Actor, current_rank, endpoint, proc_mesh\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define the Model\n",
        "\n",
        "Let's define a simple toy model that we'll use for distributed training. This model has configurable sizes to test different scales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToyModel(nn.Module):\n",
        "    def __init__(self, input_size=128, hidden_size=512, output_size=64):\n",
        "        super(ToyModel, self).__init__()\n",
        "        self.net1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.net2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.net3 = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.net1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu2(self.net2(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.net3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Multi-Node DDP Actor\n",
        "\n",
        "This is the core component that handles distributed training across multiple nodes. The actor:\n",
        "- Manages distributed process groups\n",
        "- Coordinates between different nodes and GPUs\n",
        "- Handles the actual training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiNodeDDPActor(Actor):\n",
        "    \"\"\"\n",
        "    Multi-node DDP Actor that can run across multiple machines using MMT.\n",
        "    Adapted from the single-machine DDPActor to work in a multi-node environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gpus_per_node=8):\n",
        "        self.rank = current_rank().rank\n",
        "        self.gpus_per_node = gpus_per_node\n",
        "\n",
        "        # Get distributed environment variables set by MMT\n",
        "        self.world_size = int(os.environ.get(\"WORLD_SIZE\", gpus_per_node))\n",
        "        self.node_rank = int(os.environ.get(\"NODE_RANK\", 0))\n",
        "        self.master_addr = os.environ.get(\"MASTER_ADDR\", \"localhost\")\n",
        "        self.master_port = os.environ.get(\"MASTER_PORT\", \"12355\")\n",
        "\n",
        "        # Calculate global rank\n",
        "        self.global_rank = self.node_rank * self.gpus_per_node + self.rank\n",
        "\n",
        "    def _rprint(self, msg):\n",
        "        print(\n",
        "            f\"Node {self.node_rank}, Local Rank {self.rank}, Global Rank {self.global_rank}: {msg}\"\n",
        "        )\n",
        "\n",
        "    @endpoint\n",
        "    async def setup(self):\n",
        "        self._rprint(\"Initializing torch distributed for multi-node training\")\n",
        "        self._rprint(\n",
        "            f\"World size: {self.world_size}, Master: {self.master_addr}:{self.master_port}\"\n",
        "        )\n",
        "\n",
        "        # Initialize the process group for multi-node training\n",
        "        dist.init_process_group(\n",
        "            backend=\"nccl\",  # Use NCCL for multi-GPU/multi-node\n",
        "            rank=self.global_rank,\n",
        "            world_size=self.world_size,\n",
        "        )\n",
        "\n",
        "        # Set the device for this process\n",
        "        torch.cuda.set_device(self.rank)\n",
        "\n",
        "        self._rprint(\"Finished initializing torch distributed\")\n",
        "\n",
        "    @endpoint\n",
        "    async def cleanup(self):\n",
        "        self._rprint(\"Cleaning up torch distributed\")\n",
        "        dist.destroy_process_group()\n",
        "\n",
        "    @endpoint\n",
        "    async def demo_basic(\n",
        "        self,\n",
        "        num_epochs=10,\n",
        "        batch_size=64,\n",
        "        input_size=128,\n",
        "        hidden_size=512,\n",
        "        output_size=64,\n",
        "    ):\n",
        "        self._rprint(\"Running multi-node DDP example\")\n",
        "\n",
        "        # Create model and move it to the appropriate GPU\n",
        "        device = torch.device(f\"cuda:{self.rank}\")\n",
        "        model = ToyModel(\n",
        "            input_size=input_size, hidden_size=hidden_size, output_size=output_size\n",
        "        ).to(device)\n",
        "\n",
        "        # Wrap model with DDP\n",
        "        ddp_model = DDP(model, device_ids=[self.rank])\n",
        "\n",
        "        # Print model size information (only from rank 0)\n",
        "        if self.global_rank == 0:\n",
        "            total_params = sum(p.numel() for p in model.parameters())\n",
        "            total_size_gb = total_params * 4 / (1024**3)\n",
        "            self._rprint(\n",
        "                f\"Model has {total_params:,} parameters ({total_size_gb:.2f} GB)\"\n",
        "            )\n",
        "\n",
        "        loss_fn = nn.MSELoss()\n",
        "        optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Generate random input data\n",
        "            inputs = torch.randn(batch_size, input_size).to(device)\n",
        "            labels = torch.randn(batch_size, output_size).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = ddp_model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print progress from all ranks every 5 epochs\n",
        "            if epoch % 5 == 0:\n",
        "                self._rprint(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        self._rprint(\"Finished multi-node DDP training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Orchestration\n",
        "\n",
        "This function orchestrates the training on each node by:\n",
        "1. Setting up the process mesh\n",
        "2. Spawning the DDP actor\n",
        "3. Running the training\n",
        "4. Cleaning up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def run_multi_node_training(gpus_per_node=8):\n",
        "    \"\"\"\n",
        "    Run multi-node training using the local process mesh.\n",
        "    This function is called on each node by the MMT job.\n",
        "    \"\"\"\n",
        "    # Get the number of GPUs available on this node\n",
        "    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else gpus_per_node\n",
        "\n",
        "    print(f\"Starting training on node with {num_gpus} GPUs\")\n",
        "\n",
        "    # Create process mesh for this node\n",
        "    local_proc_mesh = await proc_mesh(\n",
        "        gpus=num_gpus,\n",
        "        env={\n",
        "            \"MASTER_ADDR\": os.environ.get(\"MASTER_ADDR\", \"localhost\"),\n",
        "            \"MASTER_PORT\": os.environ.get(\"MASTER_PORT\", \"12355\"),\n",
        "            \"WORLD_SIZE\": os.environ.get(\"WORLD_SIZE\", str(num_gpus)),\n",
        "            \"NODE_RANK\": os.environ.get(\"NODE_RANK\", \"0\"),\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Spawn our actor mesh on top of the process mesh\n",
        "    ddp_actor = await local_proc_mesh.spawn(\"ddp_actor\", MultiNodeDDPActor, num_gpus)\n",
        "\n",
        "    # Setup torch Distributed\n",
        "    await ddp_actor.setup.call()\n",
        "\n",
        "    # Run the training with larger model for multi-node setup\n",
        "    await ddp_actor.demo_basic.call(\n",
        "        num_epochs=100,\n",
        "        batch_size=256,\n",
        "        input_size=512,\n",
        "        hidden_size=1024 * 16,  # Large model that benefits from multi-node\n",
        "        output_size=256,\n",
        "    )\n",
        "\n",
        "    # Cleanup\n",
        "    await ddp_actor.cleanup.call()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Job Launching with Lightning SDK\n",
        "\n",
        "These functions handle launching and monitoring the multi-machine training job using Lightning SDK's MMT API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def launch_mmt_job(num_nodes=3, teamspace=\"my-teamspace\", user=\"my-user\"):\n",
        "    \"\"\"\n",
        "    Launch a multi-machine training job using Lightning SDK's MMT API.\n",
        "    \"\"\"\n",
        "    # Initialize a Studio\n",
        "    studio = Studio(name=\"multi-node-ddp-training\", teamspace=teamspace, user=user)\n",
        "    studio.start()\n",
        "\n",
        "    # Create the training script content that will be executed on each node\n",
        "    training_script = \"\"\"\n",
        "        import asyncio\n",
        "        import sys\n",
        "        import os\n",
        "\n",
        "        # Add the current directory to Python path if needed\n",
        "        sys.path.append(os.getcwd())\n",
        "\n",
        "        from mmt_spmd_ddp import run_multi_node_training\n",
        "\n",
        "        if __name__ == \"__main__\":\n",
        "            asyncio.run(run_multi_node_training(gpus_per_node=8))\n",
        "    \"\"\"\n",
        "\n",
        "    # Write the training script to a temporary file\n",
        "    with open(\"multi_node_train.py\", \"w\") as f:\n",
        "        f.write(training_script)\n",
        "\n",
        "    print(f\"Launching MMT job with {num_nodes} nodes...\")\n",
        "\n",
        "    # Run a Multi-machine job\n",
        "    job = MMT.run(\n",
        "        command=\"python multi_node_train.py\",\n",
        "        name=\"multi-node-ddp-training\",\n",
        "        machine=Machine.T4,  # Use GPU machines for training\n",
        "        studio=studio,\n",
        "        num_machines=num_nodes,\n",
        "        env={\n",
        "            \"CUDA_VISIBLE_DEVICES\": \"0,1,2,3,4,5,6,7\",  # Make all GPUs visible\n",
        "        },\n",
        "    )\n",
        "\n",
        "    print(f\"Job started with ID: {job.name}\")\n",
        "    print(f\"Job status: {job.status}\")\n",
        "\n",
        "    # Monitor job status\n",
        "    return job, studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def monitor_job(job, studio):\n",
        "    \"\"\"\n",
        "    Monitor the job status and provide updates.\n",
        "    \"\"\"\n",
        "    import time\n",
        "\n",
        "    print(\"Monitoring job status...\")\n",
        "    while job.status in [\"Running\", \"Pending\"]:\n",
        "        print(f\"Job status: {job.status}\")\n",
        "        time.sleep(30)  # Check every 30 seconds\n",
        "\n",
        "    print(f\"Final job status: {job.status}\")\n",
        "\n",
        "    # Clean up\n",
        "    if job.status == \"Completed\":\n",
        "        print(\"Training completed successfully!\")\n",
        "    else:\n",
        "        print(f\"Training finished with status: {job.status}\")\n",
        "\n",
        "    # Shut down the studio\n",
        "    studio.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Example Usage\n",
        "\n",
        "Here's how to launch a multi-node training job. Make sure to update the configuration parameters below with your actual values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "NUM_NODES = 3\n",
        "TEAMSPACE = \"general\"  # Replace with your teamspace\n",
        "USER = \"alisol\"  # Replace with your username\n",
        "\n",
        "# Launch the job\n",
        "job, studio = launch_mmt_job(\n",
        "    num_nodes=NUM_NODES, teamspace=TEAMSPACE, user=USER\n",
        ")\n",
        "\n",
        "print(f\"Job launched. You can monitor it using: job.status\")\n",
        "print(f\"To stop the job: job.stop()\")\n",
        "print(f\"To clean up: studio.stop()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Job Monitoring\n",
        "\n",
        "You can use this cell to monitor your job progress:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check job status\n",
        "# print(f\"Current job status: {job.status}\")\n",
        "\n",
        "# Uncomment to monitor the job automatically\n",
        "# monitor_job(job, studio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Local Testing\n",
        "\n",
        "For testing purposes, you can also run the training locally (single node) by executing the training function directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Local testing - uncomment to run single-node training\n",
        "# import os\n",
        "#\n",
        "# # Set environment variables for local testing\n",
        "# os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "# os.environ[\"MASTER_PORT\"] = \"12355\"\n",
        "# os.environ[\"WORLD_SIZE\"] = \"1\"\n",
        "# os.environ[\"NODE_RANK\"] = \"0\"\n",
        "#\n",
        "# # Run local training\n",
        "# await run_multi_node_training(gpus_per_node=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Concepts\n",
        "\n",
        "### Multi-Machine Training (MMT)\n",
        "- Distributed computing across multiple physical machines\n",
        "- Managed by Lightning SDK for resource allocation and coordination\n",
        "\n",
        "### SPMD (Single Program, Multiple Data)\n",
        "- Same program runs on all nodes/processes\n",
        "- Each process works on different data\n",
        "- Coordination through message passing\n",
        "\n",
        "### Distributed Data Parallel (DDP)\n",
        "- PyTorch's method for distributed training\n",
        "- Model replicated across devices\n",
        "- Gradients synchronized across all replicas\n",
        "\n",
        "### Actor Model\n",
        "- Monarch's abstraction for distributed computation\n",
        "- Encapsulates state and behavior\n",
        "- Communicates through message passing\n",
        "\n",
        "This notebook demonstrates how these technologies work together to enable scalable machine learning training across multiple machines."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "monarch_env (local)",
      "language": "python",
      "name": "monarch_env_local"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
