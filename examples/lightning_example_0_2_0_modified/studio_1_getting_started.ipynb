{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studio 1: Getting Started - Multi-Node Training with Monarch & Lightning\n",
    "\n",
    "Welcome! This notebook will guide you through running **distributed multi-node training** using **Monarch** (Meta's distributed actor framework) with **TorchTitan** (PyTorch's large-scale LLM training library) on **Lightning AI** infrastructure.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./assets/NB_Monarch_Lightning.svg\" alt=\"Monarch Lightning Architecture\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll:\n",
    "- Set up TorchTitan, Monarch, and Lightning SDK\n",
    "- Launch a multi-node training job on Lightning AI\n",
    "- Run distributed Llama-3-8B training across multiple GPUs\n",
    "- Monitor and manage your distributed training\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Monarch Basics**: New to Monarch? Start with [Studio 0: Monarch Basics](./studio_0_monarch_basics.ipynb) to learn about Actors, Endpoints, and Process Meshes\n",
    "- Lightning AI account with access to GPU machines (L40S recommended)\n",
    "- Hugging Face account with Llama model access\n",
    "- Basic understanding of distributed training concepts\n",
    "\n",
    "## Lightning Studios Series\n",
    "\n",
    "This is **Studio 1** of the series:\n",
    "\n",
    "- **[Studio 0: Monarch Basics](./studio_0_monarch_basics.ipynb)** - Learn Monarch fundamentals (Start here if new!)\n",
    "- **Studio 1: Getting Started** - Multi-node training (YOU ARE HERE)\n",
    "- **[Studio 2: Workspace Sync](./studio_2_workspace_sync.ipynb)** - Hot-reload configs without restarting\n",
    "- **[Studio 3: Interactive Debugging](./studio_3_interactive_debugging.ipynb)** - Debug distributed systems\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part I: Environment Setup\n",
    "\n",
    "Before running distributed training, we need to install dependencies. Follow the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TorchTitan\n",
    "\n",
    "Clone the TorchTitan repository and install from PyPI:\n",
    "\n",
    "```bash\n",
    "# Clone the repository (needed for config files, scripts, and test assets)\n",
    "git clone https://github.com/pytorch/torchtitan.git\n",
    "cd torchtitan\n",
    "\n",
    "# Install TorchTitan from PyPI\n",
    "pip install torchtitan\n",
    "```\n",
    "\n",
    "Note: TorchTitan requires PyTorch. If you need a specific CUDA version, install PyTorch first:\n",
    "\n",
    "```bash\n",
    "pip install torch --index-url https://download.pytorch.org/whl/cu126\n",
    "pip install torchtitan\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Llama-3-8B Model Assets\n",
    "\n",
    "Download the Llama-3.1-8B tokenizer from Hugging Face. You'll need a Hugging Face token with access to the Llama models:\n",
    "\n",
    "```bash\n",
    "python scripts/download_hf_assets.py \\\n",
    "    --repo_id meta-llama/Llama-3.1-8B \\\n",
    "    --assets tokenizer \\\n",
    "    --hf_token=YOUR_HUGGINGFACE_TOKEN_KEY\n",
    "```\n",
    "\n",
    "Replace `YOUR_HUGGINGFACE_TOKEN_KEY` with your actual Hugging Face token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Monarch\n",
    "\n",
    "Install Monarch (torchmonarch) version 0.2.0 from PyPI:\n",
    "\n",
    "```bash\n",
    "pip install torchmonarch==0.3.0\n",
    "```\n",
    "\n",
    "For more information, visit: https://github.com/meta-pytorch/monarch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Weights & Biases\n",
    "\n",
    "Install wandb for experiment tracking:\n",
    "\n",
    "```bash\n",
    "pip install wandb\n",
    "wandb login\n",
    "```\n",
    "\n",
    "Follow the prompts to authenticate with your wandb account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Lightning SDK\n",
    "\n",
    "Install the latest version of Lightning SDK for IP sharing features:\n",
    "\n",
    "```bash\n",
    "pip install -U lightning_sdk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Installations\n",
    "\n",
    "After completing the installation steps above, verify that TorchTitan and Monarch are properly installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify TorchTitan installation\n",
    "import torchtitan\n",
    "print(\"TorchTitan is installed successfully\")\n",
    "\n",
    "# Verify Monarch installation\n",
    "import monarch\n",
    "print(\"Monarch is installed successfully\")\n",
    "\n",
    "# Verify PyTorch and CUDA\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part II: Multi-Node Training with Monarch and Lightning\n",
    "\n",
    "Now that the environment is set up, we'll configure and launch distributed training across multiple nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Environment and Imports\n",
    "\n",
    "Set up environment variables and import necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Need to set before importing monarch\n",
    "os.environ[\"MONARCH_FILE_LOG\"] = \"debug\"\n",
    "os.environ[\"HYPERACTOR_MESH_ENABLE_LOG_FORWARDING\"] = \"true\"\n",
    "os.environ[\"HYPERACTOR_MESH_ENABLE_FILE_CAPTURE\"] = \"true\"\n",
    "os.environ[\"HYPERACTOR_MESH_TAIL_LOG_LINES\"] = \"100\"\n",
    "\n",
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from utils import get_host_ip_addr, bootstrap_addr\n",
    "from monarch.actor import Actor, enable_transport, endpoint\n",
    "from monarch._src.actor.bootstrap import attach_to_workers\n",
    "\n",
    "# Configuration\n",
    "NUM_NODES = 2\n",
    "NUM_GPUS = 8\n",
    "port = 26600\n",
    "\n",
    "# Enable client transport\n",
    "host_ip_addr = get_host_ip_addr(addr_type=\"public\")\n",
    "enable_transport(f\"tcp://{host_ip_addr}:{port}@tcp://0.0.0.0:{port}\")\n",
    "print(f\"Client transport enabled at {host_ip_addr}:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the Multi-Node Training Job\n",
    "\n",
    "Execute the launch function to start the distributed training infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_utils import launch_mmt_job\n",
    "\n",
    "MMT_JOB_NAME = f\"Monarch-v0.2.0-MMT-{NUM_NODES}-nodes\"\n",
    "\n",
    "job, studio = launch_mmt_job(\n",
    "    num_nodes=NUM_NODES,\n",
    "    mmt_job_name=MMT_JOB_NAME,\n",
    "    port=port,\n",
    "    num_gpus=NUM_GPUS,\n",
    ")\n",
    "\n",
    "print(f\"Job launched. You can monitor it using: job.status\")\n",
    "print(f\"To stop the job: job.stop()\")\n",
    "print(f\"To clean up: studio.stop()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Job Status\n",
    "\n",
    "You can monitor your job through the MMT plugin in Lightning AI. The nodes will go through these stages:\n",
    "\n",
    "1. **Pending** - Waiting for resources\n",
    "2. **Setting up** - Installing dependencies and snapshotting environment\n",
    "3. **Running** - All nodes ready with bootstrap process running\n",
    "\n",
    "Wait for all nodes to show **Running** status before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check job status\n",
    "print(f\"Current job status: {job.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Process Mesh from Workers\n",
    "\n",
    "Get worker IP addresses from the job and create a process mesh by attaching to the workers.\n",
    "\n",
    "> **Important:** Make sure the bootstrap process is running on all nodes before running this cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_sdk import Status\n",
    "\n",
    "if job.status == Status('Running'):\n",
    "    # Get worker IP addresses from the job\n",
    "    ip_addresses_list_public = [machine.public_ip for machine in job.machines]\n",
    "    print(f\"Worker IPs: {ip_addresses_list_public}\")\n",
    "\n",
    "    # Create worker addresses\n",
    "    worker_addrs = [f\"tcp://{ip}:{port}@tcp://0.0.0.0:{port}\" for ip in ip_addresses_list_public]\n",
    "    print(f\"Worker addresses: {worker_addrs}\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        f\"Job status is {job.status}; however the status should be {Status('Running')} to initiate the mesh\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach to workers and create process mesh\n",
    "host_mesh = attach_to_workers(\n",
    "    name=\"host_mesh\", ca=\"trust_all_connections\", workers=worker_addrs\n",
    ")\n",
    "\n",
    "proc_mesh = host_mesh.spawn_procs(per_host={\"gpus\": NUM_GPUS})\n",
    "await proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)\n",
    "\n",
    "print(f\"\\nProcess mesh initialized successfully!\")\n",
    "print(f\"Created with {NUM_NODES} nodes and {NUM_GPUS} GPUs per node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Run TorchTitan Training for Llama-3-8B\n",
    "\n",
    "Now we'll define a Monarch Actor that wraps TorchTitan's training functionality and run distributed training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Job Name Helper\n",
    "\n",
    "Create a unique job name for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "def get_job_name(num_hosts: int, num_gpus_per_host: int):\n",
    "    return f\"monarch-{getpass.getuser()}-hosts{num_hosts}-gpus{num_gpus_per_host}\"\n",
    "\n",
    "print(get_job_name(num_hosts=NUM_NODES, num_gpus_per_host=NUM_GPUS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define TorchTitan Trainer Actor\n",
    "\n",
    "Create the `TitanTrainerWrapper` class, a Monarch Actor that wraps TorchTitan's training functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from monarch.actor import ProcMesh, Actor, endpoint, current_rank\n",
    "import socket\n",
    "from torchtitan.tools.logging import init_logger, logger\n",
    "from torchtitan.train import Trainer\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torchtitan.config import JobConfig\n",
    "\n",
    "\n",
    "class TitanTrainerWrapper(Actor):\n",
    "    def __init__(self, job_config: JobConfig):\n",
    "        self.rank = current_rank().rank\n",
    "        self.job_config = job_config\n",
    "\n",
    "    def _rprint(self, msg):\n",
    "        \"\"\"Helper method to print with rank information.\"\"\"\n",
    "        print(f\"{self.rank=} {msg}\")\n",
    "\n",
    "    @endpoint\n",
    "    def init(self):\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stderr))\n",
    "        print(f\"Initializing actor: {self.rank} {current_rank()=} {socket.gethostname()=}\")\n",
    "\n",
    "    @endpoint\n",
    "    def train(self):\n",
    "        logger.info(\"Starting training\")\n",
    "        config = self.job_config\n",
    "        trainer: Optional[Trainer] = None\n",
    "\n",
    "        try:\n",
    "            trainer = Trainer(config)\n",
    "            trainer.train()\n",
    "\n",
    "            if config.checkpoint.create_seed_checkpoint:\n",
    "                assert (\n",
    "                    int(os.environ[\"WORLD_SIZE\"]) == 1\n",
    "                ), \"Must create seed checkpoint using a single device, to disable sharding.\"\n",
    "                assert config.checkpoint.enable, \"Must enable checkpointing when creating a seed checkpoint.\"\n",
    "                trainer.checkpointer.save(curr_step=0)\n",
    "                logger.info(\"Created seed checkpoint\")\n",
    "            else:\n",
    "                trainer.train()\n",
    "        finally:\n",
    "            if trainer:\n",
    "                trainer.close()\n",
    "\n",
    "            if torch.distributed.is_initialized():\n",
    "                torch.distributed.destroy_process_group()\n",
    "                logger.info(\"Process group destroyed.\")\n",
    "        print(\"Done training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Async Main Training Function\n",
    "\n",
    "Set up the main asynchronous function that orchestrates distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtitan.config import ConfigManager, JobConfig\n",
    "from monarch.tools.network import AddrType\n",
    "from monarch.utils import setup_env_for_distributed\n",
    "\n",
    "\n",
    "async def async_main(job_config: JobConfig):\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "    # Use IPv4 for MASTER_ADDR\n",
    "    await setup_env_for_distributed(proc_mesh, use_ipaddr=AddrType.IPv4)\n",
    "\n",
    "    await proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)\n",
    "\n",
    "    print(job_config)\n",
    "    print(f\"Spawning meshes on {job_name}\")\n",
    "\n",
    "    trainer_actor = proc_mesh.spawn(\"trainer_actor\", TitanTrainerWrapper, job_config)\n",
    "\n",
    "    await trainer_actor.init.call()\n",
    "    await trainer_actor.train.call()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Logger and Run Training\n",
    "\n",
    "Configure the TorchTitan logger, set up training parameters, and execute the training pipeline.\n",
    "\n",
    "> **Note:** This will train Llama-3-8B for 25 steps. Adjust the paths below to match your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logger()\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "manual_args = [\n",
    "    \"--job.config_file\",\n",
    "    os.path.expanduser(\"/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml\"),\n",
    "    \"--model.tokenizer-path\",\n",
    "    \"/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B\",\n",
    "    \"--training.steps\",\n",
    "    \"25\",\n",
    "    \"--training.dataset_path\",\n",
    "    \"/teamspace/studios/this_studio/torchtitan/tests/assets/c4_test\",\n",
    "    \"--job.dump_folder\",\n",
    "    \"/teamspace/studios/this_studio/torchtitan/outputs/\" + job_name,\n",
    "    \"--training.seq_len\",\n",
    "    \"1024\",\n",
    "]\n",
    "config = config_manager.parse_args(manual_args)\n",
    "await async_main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Congratulations!\n",
    "\n",
    "You just ran **interactive distributed training** for a Llama-3-8B model in a Jupyter notebook using **Monarch actors** and **Lightning infrastructure**!\n",
    "\n",
    "## What You Accomplished\n",
    "\n",
    "- Launched a multi-node training job on Lightning AI\n",
    "- Set up a distributed process mesh with Monarch\n",
    "- Ran TorchTitan training across multiple GPUs and nodes\n",
    "- Monitored training with aggregated logging\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "- **Flexibility**: Change configurations and relaunch training without restarting nodes\n",
    "- **Observability**: Monarch aggregates logs from all ranks\n",
    "- **Scalability**: Easily scale from 2 to 16+ nodes by changing `NUM_NODES`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you've mastered multi-node training, continue with the Lightning Studios series:\n",
    "\n",
    "### Studio 2: Workspace Synchronization (Recommended Next!)\n",
    "**[studio_2_workspace_sync.ipynb](./studio_2_workspace_sync.ipynb)**\n",
    "\n",
    "Learn how to:\n",
    "- Sync local code/config changes to remote nodes **without restarting**\n",
    "- Hot-reload training configurations\n",
    "- Iterate faster on distributed training (10x speedup!)\n",
    "\n",
    "### Studio 3: Interactive Debugging\n",
    "**[studio_3_interactive_debugging.ipynb](./studio_3_interactive_debugging.ipynb)**\n",
    "\n",
    "Master advanced debugging:\n",
    "- Set breakpoints in distributed actors\n",
    "- Debug specific ranks interactively\n",
    "- Inspect environment variables across nodes\n",
    "\n",
    "### Review Monarch Basics\n",
    "**[studio_0_monarch_basics.ipynb](./studio_0_monarch_basics.ipynb)**\n",
    "\n",
    "If you want to review Monarch fundamentals:\n",
    "- Actors, Endpoints, and Process Meshes\n",
    "- Calling patterns (`.call()` vs `.call_one()`)\n",
    "- Actor-to-actor communication\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "When you're done, remember to stop the process mesh and clean up resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the host mesh\n",
    "host_mesh.shutdown().get()\n",
    "\n",
    "# Stop the Lightning job\n",
    "job.stop()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
