{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hero Notebook: TorchTitan Multi-Node Training with Monarch & Lightning SDK\n",
    "\n",
    "This notebook demonstrates how to run TorchTitan training using Monarch for distributed multi-node training on Lightning AI infrastructure.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./assets/NB_Monarch_Lightning.svg\" alt=\"Monarch Lightning Architecture\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "<!-- Image size settings:\n",
    "  - Adjust 'width' attribute to control the diagram size (e.g., width=\"600\", width=\"1000\", or width=\"100%\")\n",
    "  - You can also use 'height' attribute instead (e.g., height=\"400\")\n",
    "  - Remove width/height attributes to display at original size\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "This notebook provides a comprehensive guide to running distributed multi-node training using **Monarch** (Meta's distributed actor framework) with **TorchTitan** (PyTorch's large-scale LLM training library) on **Lightning AI** infrastructure. You'll learn how to set up, execute, debug, and manage distributed training workflows across multiple GPU nodes. \n",
    "\n",
    "While Part I & II are the core of this Notebook for setup and training; Part III is for users who are interested in Monarch's advanced features such as interactive distributed debugging, environment variable management, and code synchronization for workspaces between local node and remote nodes.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "**Part I: Environment Setup** *(Essential Prerequisites)*\n",
    "- Install TorchTitan - Set up PyTorch and TorchTitan for LLM training\n",
    "- Download Llama-3.1-8B Model Assets - Get model tokenizers from Hugging Face\n",
    "- Install Monarch - Install Meta's distributed actor framework\n",
    "- Setup Weights & Biases - Configure experiment tracking\n",
    "- Update Lightning SDK - Get the latest Lightning SDK features\n",
    "- Verify Installations - Confirm all dependencies are ready\n",
    "\n",
    "**Part II: Multi-Node Training** *(Core Training Workflow)*\n",
    "- Import Lightning SDK Components - Import required classes for multi-machine training\n",
    "- Configure Training Job Parameters - Set up nodes, GPUs, and network settings\n",
    "- Launch Multi-Node Training Job - Start distributed infrastructure on Lightning AI\n",
    "- Set Up Process Mesh - Initialize Monarch's distributed computing mesh\n",
    "- Define TorchTitan Trainer Actor - Create distributed training actor\n",
    "- Run TorchTitan Training - Execute Llama 3-8B training across nodes\n",
    "\n",
    "**Part III: Advanced Features** *(Distributed Development & Debugging)*\n",
    "\n",
    "1. **Environment Variable Management**\n",
    "   - Spawn Environment Variable Actor - Manage env vars across nodes\n",
    "   - Get/Set Environment Variables - Inspect and modify remote environments\n",
    "   - List Environment Variables - Query env vars by prefix\n",
    "\n",
    "2. **Workspace Synchronization** *(Hot-Reload Code & Configs)*\n",
    "   - Introduction to sync_workspace - Understanding workspace sync\n",
    "   - Content checker Actor for files - Define an Actor to check content\n",
    "   - Create Local Configuration - Set up training configs\n",
    "   - Sync to Remote Nodes - Propagate changes to workers\n",
    "   - Verify Synchronization - Confirm files are synced\n",
    "\n",
    "3. **Interactive Debugging with Breakpoints**\n",
    "   - Debugging Overview - Using pdb with distributed actors\n",
    "   - Define Debug Trainer - Create actor with breakpoints\n",
    "   - Spawn and Debug - Run interactive debugging session\n",
    "   - Debugger Commands - Learn monarch debug CLI commands\n",
    "\n",
    "**Part IV: Cleanup**\n",
    "- Stop Process Mesh - Gracefully shutdown distributed resources\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Monarch Actor**: Distributed computation unit that runs on remote nodes\n",
    "- **Process Mesh (ProcMesh)**: Network of processes across multiple nodes for distributed computing\n",
    "- **Endpoint**: Method decorator that makes actor methods callable remotely\n",
    "- **Workspace Sync**: Synchronize local code/config changes to remote worker nodes without restart\n",
    "- **Lightning MMT**: Multi-Machine Training orchestration on Lightning AI\n",
    "\n",
    "### Prerequisites\n",
    "- Lightning AI account with access to GPU machines (L40S recommended)\n",
    "- Hugging Face account with Llama model access\n",
    "- Basic understanding of distributed training concepts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Environment Setup\n",
    "\n",
    "Before running the notebook cells, ensure all dependencies are properly installed by following the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TorchTitan\n",
    "\n",
    "Clone the TorchTitan repository, install the nightly PyTorch build with CUDA 12.6 support, and install TorchTitan:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/pytorch/torchtitan.git\n",
    "cd torchtitan\n",
    "pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu126 --force-reinstall\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Llama-3-8B Model Assets\n",
    "\n",
    "Download the Llama-3.1-8B tokenizer from Hugging Face. You'll need a Hugging Face token with access to the Llama models:\n",
    "\n",
    "```bash\n",
    "python scripts/download_hf_assets.py \\\n",
    "    --repo_id meta-llama/Llama-3.1-8B \\\n",
    "    --assets tokenizer \\\n",
    "    --hf_token=YOUR_HUGGINGFACE_TOKEN_KEY\n",
    "```\n",
    "\n",
    "Replace `YOUR_HUGGINGFACE_TOKEN_KEY` with your actual Hugging Face token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Monarch\n",
    "\n",
    "Install Monarch from the GitHub repository following the Ubuntu installation instructions:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/meta-pytorch/monarch.git\n",
    "cd monarch\n",
    "# Follow the Ubuntu installation instructions from the repository\n",
    "```\n",
    "\n",
    "For detailed installation steps, visit: https://github.com/meta-pytorch/monarch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Weights & Biases\n",
    "\n",
    "Check if wandb is installed. If not, install it and login:\n",
    "\n",
    "```bash\n",
    "pip install wandb\n",
    "wandb login\n",
    "```\n",
    "\n",
    "Follow the prompts to authenticate with your wandb account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Lightning SDK\n",
    "\n",
    "The latest version of lightning SDK offers IP sharing between the client host and remote nodes. This features is being used in this Notebook.\n",
    "\n",
    "```bash\n",
    "pip install -U lightning_sdk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Installations\n",
    "\n",
    "After completing the installation steps above, verify that TorchTitan and Monarch are properly installed:\n",
    "\n",
    "```python\n",
    "# Verify TorchTitan installation\n",
    "import torchtitan\n",
    "print(\"TorchTitan is installed successfully\")\n",
    "\n",
    "# Verify Monarch installation\n",
    "import monarch\n",
    "print(\"Monarch is installed successfully\")\n",
    "\n",
    "# Verify PyTorch and CUDA\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "```\n",
    "\n",
    "If all imports succeed, you're ready to proceed with the training workflow below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part II: Multi-Node Training with Monarch and Lightning\n",
    "\n",
    "Now that the environment is set up, we can proceed with configuring and launching the distributed training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Lightning SDK Components\n",
    "\n",
    "Import the necessary classes from Lightning SDK to manage multi-machine training jobs, including `Machine` for hardware specifications, `MMT` for multi-machine training orchestration, and `Studio` for workspace management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_sdk import Machine, MMT, Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training Job Parameters\n",
    "\n",
    "Set up the configuration for the multi-node training job, including the number of nodes (2), GPUs per node (8), teamspace name, username, and port range for worker node communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "NUM_NODES = 2\n",
    "NUM_GPUS = 8\n",
    "TEAMSPACE = \"general\"  # Replace with your teamspace\n",
    "USER = \"meta-ai\"  # Replace with your username\n",
    "MMT_JOB_NAME = f\"Monarch-v0-MMT-{NUM_NODES}-nodes\"\n",
    "\n",
    "# Remote allowed port range for worker nodes\n",
    "REMOTE_ALLOWED_PORT_RANGE = \"26601..26611\"\n",
    "\n",
    "# To force Monarch to use V0 for this Notebook (This will be removed in the future)\n",
    "os.environ[\"MONARCH_V0_WORKAROUND_DO_NOT_USE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MMT Job Launch Function\n",
    "\n",
    "Create a function to launch a multi-machine training (MMT) job using Lightning SDK. This function installs the MMT plugin, configures the machine type (L40S GPUs), sets environment variables for CUDA devices and Monarch configurations, and returns the job handle and studio instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_mmt_job(num_nodes=2, teamspace=\"my-teamspace\", user=\"my-user\"):\n",
    "    \"\"\"\n",
    "    Launch a multi-machine training job using Lightning SDK's MMT API.\n",
    "    \"\"\"\n",
    "\n",
    "    studio = Studio()\n",
    "\n",
    "    # Install the MMT plugin befor running the actual job\n",
    "    studio.install_plugin(\"multi-machine-training\")\n",
    "\n",
    "    print(f\"Launching MMT job with {num_nodes} nodes...\")\n",
    "\n",
    "    # Machine with T4 GPUs\n",
    "    # machine_type = getattr(Machine, f\"T4_X_{NUM_GPUS}\")\n",
    "\n",
    "     # Machine with L40 GPUs\n",
    "    # machine_type = getattr(Machine, f\"L4_X_{NUM_GPUS}\")\n",
    "\n",
    "    # Machine with L40S GPUs\n",
    "    machine_type = getattr(Machine, f\"L40S_X_{NUM_GPUS}\")\n",
    "\n",
    "    job = MMT.run(\n",
    "        command=\"process_allocator\",\n",
    "        name=f\"Multi-Node-Monarch-Titan-Scale-{NUM_NODES}_nodes-port_override\",\n",
    "        machine=machine_type,\n",
    "        studio=studio,\n",
    "        num_machines=num_nodes,\n",
    "        env={\n",
    "            \"CUDA_VISIBLE_DEVICES\": \"0,1,2,3,4,5,6,7\",  # Make all GPUs visible # TODO: Should make this one dynamic\n",
    "            \"MONARCH_FILE_LOG\": \"debug\",\n",
    "            \"HYPERACTOR_REMOTE_ALLOC_ALLOWED_PORT_RANGE\": REMOTE_ALLOWED_PORT_RANGE,\n",
    "            \"HYPERACTOR_REMOTE_ALLOC_BIND_TO_INADDR_ANY\": \"true\",\n",
    "            \"WORKSPACE_DIR\": \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Job started with ID: {job.name}\")\n",
    "    print(f\"Job status: {job.status}\")\n",
    "\n",
    "    # Monitor job status\n",
    "    return job, studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the Multi-Node Training Job\n",
    "\n",
    "Execute the `launch_mmt_job` function with the specified number of nodes, teamspace, and user credentials. This starts the distributed training infrastructure and provides commands for monitoring and stopping the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching MMT job with 2 nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Multi-Machine Job was successfully launched. View it at https://lightning.ai/meta-ai/general/jobs/Multi-Node-Monarch-Titan-Scale-2_nodes-port_override-7rxo2?app_id=mmt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job started with ID: Multi-Node-Monarch-Titan-Scale-2_nodes-port_override-7rxo2\n",
      "Job status: Pending\n",
      "Job launched. You can monitor it using: job.status\n",
      "To stop the job: job.stop()\n",
      "To clean up: studio.stop()\n"
     ]
    }
   ],
   "source": [
    "# Launch the job\n",
    "job, studio = launch_mmt_job(\n",
    "    num_nodes=NUM_NODES, teamspace=TEAMSPACE, user=USER\n",
    ")\n",
    "\n",
    "print(f\"Job launched. You can monitor it using: job.status\")\n",
    "print(f\"To stop the job: job.stop()\")\n",
    "print(f\"To clean up: studio.stop()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Process Mesh from Job\n",
    "\n",
    "Initialize the Monarch process mesh using the launched Lightning job. This creates the distributed computing mesh that connects all nodes and GPUs for coordinated training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mesh_utils import setup_proc_mesh_from_job\n",
    "\n",
    "proc_mesh = setup_proc_mesh_from_job(job, NUM_NODES, NUM_GPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Hero - Run TorchTitan using Monarch for Llama 3 - 8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Job Name Helper\n",
    "\n",
    "Define a utility function to generate a unique job name based on the username, number of hosts, and GPUs per host. This helps identify and track different training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "def get_job_name(num_hosts: int, num_gpus_per_host: int):\n",
    "    return f\"monarch-{getpass.getuser()}-hosts{num_hosts}-gpus{num_gpus_per_host}\"\n",
    "print(get_job_name(num_hosts=NUM_NODES, num_gpus_per_host=NUM_GPUS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define TorchTitan Trainer Actor\n",
    "\n",
    "Create the `TitanTrainerWrapper` class, a Monarch Actor that wraps TorchTitan's training functionality. This actor handles initialization, training execution, checkpointing, and cleanup of the distributed training process across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from monarch.actor import ProcMesh, Actor, endpoint, current_rank\n",
    "import socket\n",
    "from torchtitan.tools.logging import init_logger, logger\n",
    "from torchtitan.train import Trainer\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torchtitan.config import JobConfig\n",
    "\n",
    "\n",
    "class TitanTrainerWrapper(Actor):\n",
    "    def __init__(self, job_config: JobConfig):\n",
    "        self.rank = current_rank().rank\n",
    "        self.job_config = job_config\n",
    "\n",
    "    def _rprint(self, msg):\n",
    "        \"\"\"Helper method to print with rank information.\"\"\"\n",
    "        print(f\"{self.rank=} {msg}\")\n",
    "\n",
    "    @endpoint\n",
    "    def init(self):\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stderr))\n",
    "        print(f\"Initializing actor: {self.rank} {current_rank()=} {socket.gethostname()=}\")\n",
    "\n",
    "\n",
    "    @endpoint\n",
    "    def train(self):\n",
    "        logger.info(\"Starting training\")\n",
    "        config = self.job_config\n",
    "        trainer: Optional[Trainer] = None\n",
    "\n",
    "        try:\n",
    "            trainer = Trainer(config)\n",
    "            trainer.train()\n",
    "\n",
    "            if config.checkpoint.create_seed_checkpoint:\n",
    "                assert (\n",
    "                    int(os.environ[\"WORLD_SIZE\"]) == 1\n",
    "                ), \"Must create seed checkpoint using a single device, to disable sharding.\"\n",
    "                assert (\n",
    "                    # config.checkpoint.enable_checkpoint\n",
    "                    config.checkpoint.enable\n",
    "                ), \"Must enable checkpointing when creating a seed checkpoint.\"\n",
    "                trainer.checkpointer.save(curr_step=0, )\n",
    "                logger.info(\"Created seed checkpoint\")\n",
    "            else:\n",
    "                trainer.train()\n",
    "        finally:\n",
    "            if trainer:\n",
    "                trainer.close()\n",
    "\n",
    "            if torch.distributed.is_initialized():\n",
    "                torch.distributed.destroy_process_group()\n",
    "                logger.info(\"Process group destroyed.\")\n",
    "        print(\"Done training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Async Main Training Function\n",
    "\n",
    "Set up the main asynchronous function that orchestrates the distributed training. This function configures the environment for distributed execution, spawns trainer actors across the process mesh, and initiates the training workflow. The reason that this function is defined as async is becuase of those call of endpoints where need to be awaited. This makes sure that coordination of operations across multiple machines are done asynchronously rather than blocking the main thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtitan.config import ConfigManager, JobConfig\n",
    "from monarch.utils import setup_env_for_distributed\n",
    "\n",
    "async def async_main(job_config: JobConfig):\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "    \"\"\"\n",
    "    # if use_ipaddr is not passed, then default is IPv6 for MASTER_ADDR\n",
    "    \"\"\"\n",
    "    await setup_env_for_distributed(proc_mesh, use_ipaddr=AddrType.IPv4)\n",
    "\n",
    "    await proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)\n",
    "\n",
    "    print(job_config)\n",
    "    print(f\"Spawning meshes on {job_name}\")\n",
    "\n",
    "    trainer_actor = proc_mesh.spawn(\"trainer_actor\", TitanTrainerWrapper, job_config)\n",
    "\n",
    "    await trainer_actor.init.call()\n",
    "    await trainer_actor.train.call()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Logger and Run Training\n",
    "\n",
    "Configure the TorchTitan logger and parse training arguments including model configuration file, tokenizer path, dataset location, number of training steps, and output directory. Then execute the asynchronous training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logger()\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "manual_args = [\n",
    "        \"--job.config_file\",\n",
    "        os.path.expanduser(\"/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml\"),\n",
    "        \"--model.tokenizer-path\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B\",\n",
    "        \"--training.steps\",\n",
    "        \"25\",\n",
    "        \"--training.dataset_path\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/tests/assets/c4_test\",\n",
    "        \"--job.dump_folder\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/outputs/\" + job_name,\n",
    "        \"--training.seq_len\",\n",
    "        \"1024\",\n",
    "        # \"8192\",\n",
    "    ]\n",
    "config = config_manager.parse_args(manual_args)\n",
    "await async_main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🎉🎉 Congratulations!!!! 🎉🎉 You just ran the interactive distributed training for Llama-3 model in a Notebook using Monarch actors and Lightning setup!**\n",
    "\n",
    "This already gives the user lots of flexibilities such as changing the configurations and launching another training without iniatiating another job or set of nodes; or experiencing the logging aggregation using Monarch.\n",
    "\n",
    "However, a curious user can dig more into advanced features of Monarch in Part III. Monarch offers features such as interactive distributed debugging while your training is running on mutliple nodes and ranks. Another feature is the `workspace_sync` where users can update packages, environments and files and sync them with remote nodes. Without Monarch, users may need to re-initiate their launches which usually takes lots of times. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Part III: Advanced Features (Distributed Development & Debugging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variable Management with Remote Actors\n",
    "\n",
    "Spawn an actor that can interact with environment variables on remote nodes. This is useful for debugging, configuration management, and runtime environment inspection across the distributed system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monarch.actor import Actor, endpoint, current_rank\n",
    "import os\n",
    "import socket\n",
    "\n",
    "class EnvVarActor(Actor):\n",
    "    \"\"\"Actor for managing environment variables on remote nodes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rank = current_rank().rank\n",
    "        self.hostname = socket.gethostname()\n",
    "\n",
    "    @endpoint\n",
    "    def get_env(self, var_name: str) -> dict:\n",
    "        \"\"\"Get an environment variable value from the remote node.\"\"\"\n",
    "        value = os.environ.get(var_name)\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"var_name\": var_name,\n",
    "            \"value\": value\n",
    "        }\n",
    "\n",
    "    @endpoint\n",
    "    def set_env(self, var_name: str, var_value: str) -> dict:\n",
    "        \"\"\"Set an environment variable on the remote node.\"\"\"\n",
    "        os.environ[var_name] = var_value\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"var_name\": var_name,\n",
    "            \"value\": var_value,\n",
    "            \"status\": \"set\"\n",
    "        }\n",
    "\n",
    "    @endpoint\n",
    "    def list_env_vars(self, prefix: str = \"\") -> dict:\n",
    "        \"\"\"List all environment variables matching a prefix.\"\"\"\n",
    "        matching_vars = {k: v for k, v in os.environ.items() if k.startswith(prefix)}\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"matching_vars\": matching_vars,\n",
    "            \"count\": len(matching_vars)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawn the Environment Variable Actor\n",
    "\n",
    "Spawn the `EnvVarActor` across all nodes in the process mesh. Each node will have an instance that can be used to inspect and modify its local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn the environment variable actor across all nodes\n",
    "env_actor = proc_mesh.spawn(\"env_actor\", EnvVarActor)\n",
    "print(\"EnvVarActor spawned across all nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Variables from Remote Nodes\n",
    "\n",
    "Query environment variables from all remote nodes. This example retrieves the `CUDA_VISIBLE_DEVICES` variable that was set during job initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an environment variable from all nodes\n",
    "results = await env_actor.get_env.call(\"CUDA_VISIBLE_DEVICES\")\n",
    "print(\"\\nCUDA_VISIBLE_DEVICES on all nodes:\")\n",
    "for result in results:\n",
    "    print(f\"  Rank {result['rank']} ({result['hostname']}): {result['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment Variables on Remote Nodes\n",
    "\n",
    "Set a custom environment variable on all remote nodes and verify it was set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a custom environment variable on all nodes\n",
    "set_results = await env_actor.set_env.call(\"CUSTOM_VAR\", \"test_value_123\")\n",
    "print(\"\\nSetting CUSTOM_VAR on all nodes:\")\n",
    "for result in set_results:\n",
    "    print(f\"  Rank {result['rank']} ({result['hostname']}): {result['status']} - {result['value']}\")\n",
    "\n",
    "# Verify the variable was set by reading it back\n",
    "verify_results = await env_actor.get_env.call(\"CUSTOM_VAR\")\n",
    "print(\"\\nVerifying CUSTOM_VAR on all nodes:\")\n",
    "for result in verify_results:\n",
    "    print(f\"  Rank {result['rank']} ({result['hostname']}): {result['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Environment Variables with Prefix\n",
    "\n",
    "List all environment variables that match a specific prefix (e.g., all CUDA-related or MONARCH-related variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all environment variables starting with \"CUDA\"\n",
    "list_results = await env_actor.list_env_vars.call(\"CUDA\")\n",
    "print(\"\\nCUDA-related environment variables on all nodes:\")\n",
    "for result in list_results:\n",
    "    print(f\"\\n  Rank {result['rank']} ({result['hostname']}) - {result['count']} variables:\")\n",
    "    for var_name, var_value in result['matching_vars'].items():\n",
    "        print(f\"    {var_name}={var_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workspace Synchronization with `sync_workspace`\n",
    "\n",
    "When working with distributed training, you often need to modify configuration files, training scripts, or other code locally and sync those changes to remote worker nodes without restarting the entire job. Monarch's `proc_mesh.sync_workspace()` enables this workflow.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. **Make changes locally** - Edit files in your local workspace (e.g., configuration files, training scripts)\n",
    "2. **Call `sync_workspace()`** - Synchronize changes to all remote worker nodes\n",
    "3. **Continue execution** - The updated files are immediately available on all nodes\n",
    "\n",
    "This is particularly useful for:\n",
    "- Tweaking hyperparameters in configuration files\n",
    "- Updating training schedules\n",
    "- Modifying data processing logic\n",
    "- Hot-reloading code changes without job restart\n",
    "\n",
    "Let's see a practical example using TorchTitan training configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Actor to Check File Contents\n",
    "\n",
    "First, create an actor that can read and verify file contents on remote nodes. This will help us confirm that files are properly synchronized across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileCheckerActor(Actor):\n",
    "    \"\"\"Actor to read and verify file contents on remote nodes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rank = current_rank().rank\n",
    "        self.hostname = socket.gethostname()\n",
    "\n",
    "    @endpoint\n",
    "    def read_file(self, file_path: str) -> dict:\n",
    "        \"\"\"Read a file and return its contents.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            return {\n",
    "                \"rank\": self.rank,\n",
    "                \"hostname\": self.hostname,\n",
    "                \"file_path\": file_path,\n",
    "                \"content\": content,\n",
    "                \"exists\": True,\n",
    "                \"size\": len(content)\n",
    "            }\n",
    "        except FileNotFoundError:\n",
    "            return {\n",
    "                \"rank\": self.rank,\n",
    "                \"hostname\": self.hostname,\n",
    "                \"file_path\": file_path,\n",
    "                \"exists\": False,\n",
    "                \"error\": \"File not found\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"rank\": self.rank,\n",
    "                \"hostname\": self.hostname,\n",
    "                \"file_path\": file_path,\n",
    "                \"exists\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    @endpoint\n",
    "    def file_exists(self, file_path: str) -> dict:\n",
    "        \"\"\"Check if a file exists on the remote node.\"\"\"\n",
    "        exists = os.path.exists(file_path)\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"file_path\": file_path,\n",
    "            \"exists\": exists\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawn File Checker Actor\n",
    "\n",
    "Spawn the file checker actor across all nodes to verify file synchronization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn the file checker actor\n",
    "file_checker = proc_mesh.spawn(\"file_checker\", FileCheckerActor)\n",
    "print(\"FileCheckerActor spawned across all nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Local Configuration File\n",
    "\n",
    "Create a local training configuration file that we'll later modify and sync to worker nodes. This simulates a common workflow where you want to tweak hyperparameters or training settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local workspace directory for our custom config\n",
    "local_workspace = \"/teamspace/studios/this_studio/monarch_sync_example\"\n",
    "os.makedirs(local_workspace, exist_ok=True)\n",
    "\n",
    "# Create a custom training configuration file\n",
    "config_file_name = \"custom_training_config.toml\"\n",
    "local_config_path = os.path.join(local_workspace, config_file_name)\n",
    "\n",
    "# Write initial configuration\n",
    "with open(local_config_path, 'w') as f:\n",
    "    f.write(\"\"\"# TorchTitan Custom Training Configuration\n",
    "# This file demonstrates workspace synchronization\n",
    "\n",
    "[training]\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "max_steps = 100\n",
    "warmup_steps = 10\n",
    "\n",
    "[model]\n",
    "model_type = \"llama3_8b\"\n",
    "seq_len = 1024\n",
    "\n",
    "[optimizer]\n",
    "optimizer_type = \"AdamW\"\n",
    "weight_decay = 0.01\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Created local config file: {local_config_path}\")\n",
    "with open(local_config_path, 'r') as f:\n",
    "    print(f\"\\nInitial configuration:\\n{f.read()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Workspace and Perform Initial Sync\n",
    "\n",
    "Create a Monarch `Workspace` object and perform the initial synchronization to all remote worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monarch.tools.config.workspace import Workspace\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a Workspace object pointing to our local directory\n",
    "workspace = Workspace(dirs=[Path(local_workspace)])\n",
    "\n",
    "print(f\"Workspace configured: {workspace.dirs}\")\n",
    "print(f\"\\nSyncing workspace to remote nodes...\")\n",
    "\n",
    "# Perform initial sync\n",
    "await proc_mesh.sync_workspace(workspace=workspace, conda=False, auto_reload=False)\n",
    "\n",
    "print(\"Initial workspace sync completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify File on Remote Nodes\n",
    "\n",
    "Check that the configuration file was successfully synced to all remote worker nodes by reading it from each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the remote file path (files are synced to WORKSPACE_DIR)\n",
    "remote_workspace_root = os.environ.get(\"WORKSPACE_DIR\", \"/workspace\")\n",
    "remote_config_path = os.path.join(remote_workspace_root, \"monarch_sync_example\", config_file_name)\n",
    "\n",
    "print(f\"Checking file on remote nodes: {remote_config_path}\\n\")\n",
    "\n",
    "# Check file existence on all nodes\n",
    "exists_results = await file_checker.file_exists.call(remote_config_path)\n",
    "for result in exists_results:\n",
    "    status = \"EXISTS\" if result['exists'] else \" NOT FOUND\"\n",
    "    print(f\"  Rank {result['rank']} ({result['hostname']}): {status}\")\n",
    "\n",
    "# Read file content from rank 0 to verify\n",
    "print(f\"\\nReading config from rank 0:\")\n",
    "read_results = await file_checker.read_file.call(remote_config_path)\n",
    "if read_results[0]['exists']:\n",
    "    print(f\"\\n{read_results[0]['content']}\")\n",
    "else:\n",
    "    print(f\"Error: {read_results[0].get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Debugging with Breakpoints in Monarch\n",
    "\n",
    "Monarch supports interactive debugging of distributed actors using Python's built-in `pdb` debugger. You can set breakpoints in your actors, attach to specific ranks, and inspect their state during execution.\n",
    "\n",
    "### How to Debug:\n",
    "\n",
    "1. **Add breakpoints** to your actor endpoints using `breakpoint()`\n",
    "2. **Run your training** as usual - execution will pause when breakpoints are hit\n",
    "3. **Open a separate terminal** and run: `monarch debug`\n",
    "4. **Use debugger commands**:\n",
    "   - `list` - Show all active breakpoints across ranks\n",
    "   - `attach <actor_name> <rank>` - Attach to a specific actor/rank for interactive debugging\n",
    "   - `cast <actor_name> ranks(<ranks>) <pdb_command>` - Send pdb commands to multiple ranks\n",
    "   - `continue` - Resume execution\n",
    "\n",
    "Let's create a debugging example using a TorchTitan trainer with breakpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define TitanTrainerActor with Breakpoints\n",
    "\n",
    "Create a TorchTitan trainer actor with breakpoints at key stages. This allows you to inspect the training state, configuration, and execution flow interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanTrainerDebug(Actor):\n",
    "    \"\"\"TorchTitan Trainer Actor with debugging breakpoints.\"\"\"\n",
    "\n",
    "    def __init__(self, job_config: JobConfig):\n",
    "        self.rank = current_rank().rank\n",
    "        self.job_config = job_config\n",
    "        self.trainer: Optional[Trainer] = None\n",
    "\n",
    "    def _rprint(self, msg):\n",
    "        \"\"\"Helper method to print with rank information.\"\"\"\n",
    "        print(f\"{self.rank=} {msg}\")\n",
    "\n",
    "    @endpoint\n",
    "    def init(self):\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stderr))\n",
    "        self._rprint(f\"Initializing debug actor: {current_rank()=} {socket.gethostname()=}\")\n",
    "\n",
    "        # Breakpoint 1: After initialization\n",
    "        breakpoint()  # Debug: Inspect actor initialization state\n",
    "\n",
    "    @endpoint\n",
    "    def setup_trainer(self):\n",
    "        \"\"\"Setup the trainer with a breakpoint to inspect configuration.\"\"\"\n",
    "        logger.info(f\"Setting up trainer on rank {self.rank}\")\n",
    "        config = self.job_config\n",
    "\n",
    "        # Breakpoint 2: Before trainer creation\n",
    "        if self.rank == 0:  # Only break on rank 0 for simplicity\n",
    "            breakpoint()  # Debug: Inspect job config before trainer creation\n",
    "\n",
    "        self.trainer = Trainer(config)\n",
    "        self._rprint(\"Trainer setup complete\")\n",
    "\n",
    "    @endpoint\n",
    "    def train_step(self, num_steps: int = 5):\n",
    "        \"\"\"Run a few training steps with breakpoints.\"\"\"\n",
    "        if not self.trainer:\n",
    "            raise RuntimeError(\"Trainer not initialized. Call setup_trainer first.\")\n",
    "\n",
    "        logger.info(f\"Starting training for {num_steps} steps on rank {self.rank}\")\n",
    "\n",
    "        # Breakpoint 3: Before training starts\n",
    "        if self.rank == 0:\n",
    "            breakpoint()  # Debug: Inspect trainer state before training\n",
    "\n",
    "        # In a real scenario, you'd call trainer.train()\n",
    "        # For debugging purposes, we'll just simulate a few steps\n",
    "        for step in range(num_steps):\n",
    "            if step == 2 and self.rank == 0:  # Break mid-training on rank 0\n",
    "                breakpoint()  # Debug: Inspect mid-training state\n",
    "\n",
    "            self._rprint(f\"Processing step {step + 1}/{num_steps}\")\n",
    "\n",
    "        self._rprint(f\"Completed {num_steps} training steps\")\n",
    "\n",
    "    @endpoint\n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup resources.\"\"\"\n",
    "        logger.info(f\"Cleaning up trainer on rank {self.rank}\")\n",
    "\n",
    "        if self.trainer:\n",
    "            self.trainer.close()\n",
    "\n",
    "        if torch.distributed.is_initialized():\n",
    "            torch.distributed.destroy_process_group()\n",
    "            logger.info(\"Process group destroyed.\")\n",
    "\n",
    "        self._rprint(\"Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawn Debug Trainer Actor\n",
    "\n",
    "Spawn the debug trainer actor across the process mesh. When you run the following cells, execution will pause at breakpoints, allowing you to debug interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn the debug trainer actor\n",
    "debug_trainer = proc_mesh.spawn(\"debug_trainer\", TitanTrainerDebug, config)\n",
    "print(\"Debug trainer actor spawned across all nodes\")\n",
    "print(\"When breakpoints are hit, run 'monarch debug' in a separate terminal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Debug Training Session\n",
    "\n",
    "Execute the training endpoints. When breakpoints are hit:\n",
    "1. Open a separate terminal\n",
    "2. Run `monarch debug`\n",
    "3. Use `list` to see all active breakpoints\n",
    "4. Use `attach debug_trainer 0` to attach to rank 0\n",
    "5. Use standard pdb commands (`n`, `s`, `p <var>`, `l`, etc.)\n",
    "6. Use `continue` to resume execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize actors (will hit first breakpoint)\n",
    "await debug_trainer.init.call()\n",
    "\n",
    "# Setup trainer (will hit second breakpoint on rank 0)\n",
    "await debug_trainer.setup_trainer.call()\n",
    "\n",
    "# Run training steps (will hit breakpoints during training)\n",
    "await debug_trainer.train_step.call(num_steps=5)\n",
    "\n",
    "# Cleanup\n",
    "await debug_trainer.cleanup.call()\n",
    "\n",
    "print(\"Debug training session completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Debugger Commands\n",
    "\n",
    "Once in the Monarch debugger, try these commands:\n",
    "\n",
    "```bash\n",
    "# List all active breakpoints\n",
    "monarch_dbg> list\n",
    "\n",
    "# Attach to rank 0 for interactive debugging\n",
    "monarch_dbg> attach debug_trainer 0\n",
    "\n",
    "# Standard pdb commands when attached:\n",
    "(Pdb) n              # Next line\n",
    "(Pdb) s              # Step into function\n",
    "(Pdb) p self.rank    # Print variable\n",
    "(Pdb) l              # List source code\n",
    "(Pdb) c              # Continue execution\n",
    "\n",
    "# Cast commands to multiple ranks (without attaching)\n",
    "monarch_dbg> cast debug_trainer ranks(0,1) n\n",
    "monarch_dbg> cast debug_trainer ranks(0:4) c\n",
    "\n",
    "# Continue all breakpoints\n",
    "monarch_dbg> continue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup and Stop Process Mesh\n",
    "\n",
    "Gracefully stop the Monarch process mesh, cleaning up all distributed resources and shutting down the actors across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await proc_mesh.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
