{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# need to set before importing monarch\n",
    "os.environ[\"MONARCH_FILE_LOG\"] = \"debug\"\n",
    "os.environ[\"HYPERACTOR_MESH_ENABLE_LOG_FORWARDING\"] = \"true\"\n",
    "os.environ[\"HYPERACTOR_MESH_ENABLE_FILE_CAPTURE\"] = \"true\"\n",
    "os.environ[\"HYPERACTOR_MESH_TAIL_LOG_LINES\"] = \"100\"\n",
    "\n",
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from utils import get_host_ip_addr, bootstrap_addr\n",
    "from monarch.actor import Actor, enable_transport, endpoint\n",
    "from monarch._src.actor.bootstrap import attach_to_workers\n",
    "\n",
    "\n",
    "port = 26600\n",
    "host_ip_addr = get_host_ip_addr(addr_type=\"public\")\n",
    "enable_transport(f\"tcp://{host_ip_addr}:{port}@tcp://0.0.0.0:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job has not been created by the user\n",
      "Launching MMT job with 2 nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Multi-Machine Job was successfully launched. View it at https://lightning.ai/meta-ai/general/jobs/ali_titan_monarch | 0.2.0rc1 | 03?app_id=mmt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job started with ID: ali_titan_monarch | 0.2.0rc1 | 03\n",
      "Job status: Pending\n",
      "Job launched. You can monitor it using: job.status\n",
      "To stop the job: job.stop()\n",
      "To clean up: studio.stop()\n"
     ]
    }
   ],
   "source": [
    "from mmt_utils import launch_mmt_job\n",
    "\n",
    "NUM_NODES = 2\n",
    "NUM_GPUS = 8\n",
    "\n",
    "job, studio = launch_mmt_job(\n",
    "    num_nodes=NUM_NODES,\n",
    "    mmt_job_name=\"ali_titan_monarch | 0.2.0 stable\",\n",
    "    port=26600,\n",
    "    num_gpus=NUM_GPUS,\n",
    ")\n",
    "\n",
    "print(f\"Job launched. You can monitor it using: job.status\")\n",
    "print(f\"To stop the job: job.stop()\")\n",
    "print(f\"To clean up: studio.stop()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_sdk import Machine, MMT, Status, Studio\n",
    "\n",
    "\n",
    "def launch_mmt_job_titan_trainer(num_nodes=2, mmt_job_name=\"\", port=26600, num_gpus: int = 8):\n",
    "    \"\"\"\n",
    "    Launch a multi-machine training job using Lightning SDK's MMT API.\n",
    "    \"\"\"\n",
    "\n",
    "    studio = Studio()\n",
    "\n",
    "    try:\n",
    "        job = MMT(name=mmt_job_name, _fetch_job=True)\n",
    "\n",
    "        if job.status == Status(\"Running\") or job.status == Status(\"Pending\"):\n",
    "            print(\n",
    "                f\"MMT job with {num_nodes} nodes is already created! Returning the the job\"\n",
    "            )\n",
    "            return job, studio\n",
    "\n",
    "    except:\n",
    "        print(\"Job has not been created by the user\")\n",
    "\n",
    "    # Install the MMT plugin befor running the actual job\n",
    "    studio.install_plugin(\"multi-machine-training\")\n",
    "\n",
    "    print(f\"Launching MMT job with {num_nodes} nodes...\")\n",
    "\n",
    "    # Machine with T4 GPUs\n",
    "    # machine_type = getattr(Machine, f\"T4_X_{num_gpus}\")\n",
    "\n",
    "    # Machine with L4 GPUs\n",
    "    # machine_type = getattr(Machine, f\"L4_X_{num_gpus}\")\n",
    "\n",
    "    # Machine with L40S GPUs\n",
    "    # machine_type = getattr(Machine, f\"L40S_X_{num_gpus}\")\n",
    "    # python_command = f\"python -c 'from utils import bootstrap; bootstrap({port})'\"\n",
    "    # python_command = \"CONFIG_FILE='/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml' ./teamspace/studios/this_studio/torchtitan/run_train.sh\"\n",
    "\n",
    "    # Machine with T4 GPUs\n",
    "    # machine_type = getattr(Machine, f\"T4_X_{num_gpus}\")\n",
    "\n",
    "    # Machine with L4s GPUs\n",
    "    machine_type = getattr(Machine, f\"L4_X_{num_gpus}\")\n",
    "\n",
    "    # Machine with L40S GPUs\n",
    "    # machine_type = getattr(Machine, f\"L40S_X_{num_gpus}\")\n",
    "\n",
    "    python_command = \"/teamspace/studios/this_studio/torchtitan/run_train.sh\"\n",
    "    job = MMT.run(\n",
    "        command=python_command,\n",
    "        name=mmt_job_name,\n",
    "        machine=machine_type,\n",
    "        studio=studio,\n",
    "        num_machines=num_nodes,\n",
    "        env={\n",
    "            \"CONFIG_FILE\": \"/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml\",\n",
    "            \"NCCL_SOCKET_IFNAME\": \"^lo,docker\",\n",
    "            \"NCCL_IB_DISABLE\": \"1\",\n",
    "            \"NCCL_P2P_DISABLE\": \"1\",\n",
    "            \"NCCL_DEBUG\": \"INFO\",\n",
    "            \"NCCL_SOCKET_IFNAME\": \"ens5\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # os.environ[\"NCCL_SOCKET_IFNAME\"] = \"^lo,docker\"  # Use network interfaces except lo and docker\n",
    "    # os.environ[\"NCCL_IB_DISABLE\"] = \"1\"  # Disable InfiniBand (not available on GCP)\n",
    "    # os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"  # Disable P2P (can cause issues on some GCP configs)\n",
    "    # os.environ[\"NCCL_DEBUG\"] = \"INFO\"  # Enable debugging to see what NCCL is doing\n",
    "\n",
    "    print(f\"Job started with ID: {job.name}\")\n",
    "    print(f\"Job status: {job.status}\")\n",
    "\n",
    "    # Monitor job status\n",
    "    return job, studio\n",
    "\n",
    "launch_mmt_job_titan_trainer(mmt_job_name=\"Titan on GCP (AWS in PATH)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['34.169.67.140', '34.82.167.13']\n",
      "['tcp://34.169.67.140:26600@tcp://0.0.0.0:26600', 'tcp://34.82.167.13:26600@tcp://0.0.0.0:26600']\n"
     ]
    }
   ],
   "source": [
    "port = 26600\n",
    "\n",
    "ip_addresses_list_public = [machine.public_ip for machine in job.machines]\n",
    "print(ip_addresses_list_public)\n",
    "worker_addrs = [f\"tcp://{ip}:{port}@tcp://0.0.0.0:{port}\" for ip in ip_addresses_list_public]\n",
    "print(worker_addrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monarch internal logs are being written to /tmp/alisol/monarch_log.log; execution id alisol_Dec-19_05:38_192\n"
     ]
    }
   ],
   "source": [
    "host_mesh = attach_to_workers(\n",
    "    name=\"host_mesh\", ca=\"trust_all_connections\", workers=worker_addrs\n",
    ")\n",
    "\n",
    "proc_mesh = host_mesh.spawn_procs(per_host={\"gpus\": NUM_GPUS})\n",
    "await proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monarch-alisol-hosts2-gpus8\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "def get_job_name(num_hosts: int, num_gpus_per_host: int):\n",
    "    return f\"monarch-{getpass.getuser()}-hosts{num_hosts}-gpus{num_gpus_per_host}\"\n",
    "print(get_job_name(num_hosts=NUM_NODES, num_gpus_per_host=NUM_GPUS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from monarch.actor import ProcMesh, Actor, endpoint, current_rank\n",
    "import socket\n",
    "from torchtitan.tools.logging import init_logger, logger\n",
    "from torchtitan.train import Trainer\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torchtitan.config import JobConfig\n",
    "\n",
    "\n",
    "class TitanTrainerWrapper(Actor):\n",
    "    def __init__(self, job_config: JobConfig):\n",
    "        self.rank = current_rank().rank\n",
    "        self.job_config = job_config\n",
    "\n",
    "    def _rprint(self, msg):\n",
    "        \"\"\"Helper method to print with rank information.\"\"\"\n",
    "        print(f\"{self.rank=} {msg}\")\n",
    "\n",
    "    @endpoint\n",
    "    def init(self):\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stderr))\n",
    "        print(f\"Initializing actor: {self.rank} {current_rank()=} {socket.gethostname()=}\")\n",
    "\n",
    "\n",
    "    @endpoint\n",
    "    def train(self):\n",
    "        logger.info(\"Starting training\")\n",
    "        config = self.job_config\n",
    "        trainer: Optional[Trainer] = None\n",
    "\n",
    "        try:\n",
    "            trainer = Trainer(config)\n",
    "            trainer.train()\n",
    "\n",
    "            if config.checkpoint.create_seed_checkpoint:\n",
    "                assert (\n",
    "                    int(os.environ[\"WORLD_SIZE\"]) == 1\n",
    "                ), \"Must create seed checkpoint using a single device, to disable sharding.\"\n",
    "                assert (\n",
    "                    # config.checkpoint.enable_checkpoint\n",
    "                    config.checkpoint.enable\n",
    "                ), \"Must enable checkpointing when creating a seed checkpoint.\"\n",
    "                trainer.checkpointer.save(curr_step=0, )\n",
    "                logger.info(\"Created seed checkpoint\")\n",
    "            else:\n",
    "                trainer.train()\n",
    "        finally:\n",
    "            if trainer:\n",
    "                trainer.close()\n",
    "\n",
    "            if torch.distributed.is_initialized():\n",
    "                torch.distributed.destroy_process_group()\n",
    "                logger.info(\"Process group destroyed.\")\n",
    "        print(\"Done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtitan.config import ConfigManager, JobConfig\n",
    "from monarch.tools.network import AddrType\n",
    "from monarch.utils import setup_env_for_distributed\n",
    "\n",
    "async def async_main(job_config: JobConfig):\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "    \"\"\"\n",
    "    # if use_ipaddr is not passed, then default is IPv6 for MASTER_ADDR\n",
    "    \"\"\"\n",
    "    # await setup_env_for_distributed(proc_mesh,)\n",
    "    await setup_env_for_distributed(proc_mesh, use_ipaddr=AddrType.IPv4)\n",
    "\n",
    "    await proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)\n",
    "\n",
    "    print(job_config)\n",
    "    print(f\"Spawning meshes on {job_name}\")\n",
    "\n",
    "    trainer_actor = proc_mesh.spawn(\"trainer_actor\", TitanTrainerWrapper, job_config)\n",
    "\n",
    "    await trainer_actor.init.call()\n",
    "    await trainer_actor.train.call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logger()\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "job_name = get_job_name(NUM_NODES, NUM_GPUS)\n",
    "\n",
    "manual_args = [\n",
    "        \"--job.config_file\",\n",
    "        os.path.expanduser(\"/teamspace/studios/this_studio/torchtitan/torchtitan/models/llama3/train_configs/llama3_8b.toml\"),\n",
    "        \"--model.tokenizer-path\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/assets/hf/Llama-3.1-8B\",\n",
    "        \"--training.steps\",\n",
    "        \"25\",\n",
    "        \"--training.dataset\",\n",
    "        \"c4_test\",\n",
    "        \"--training.dataset_path\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/tests/assets/c4_test\",\n",
    "        \"--job.dump_folder\",\n",
    "        \"/teamspace/studios/this_studio/torchtitan/outputs/\" + job_name,\n",
    "        \"--training.seq_len\",\n",
    "        \"1024\",\n",
    "    ]\n",
    "config = config_manager.parse_args(manual_args)\n",
    "await async_main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer_actor.stop().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_mesh.shutdown().get()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
