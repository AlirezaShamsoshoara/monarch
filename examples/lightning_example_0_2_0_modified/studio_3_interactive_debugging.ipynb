{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studio 3: Interactive Debugging for Distributed Training\n",
    "\n",
    "Welcome to Studio 3! In this notebook, you'll master **interactive debugging** techniques for distributed systems using Monarch.\n",
    "\n",
    "> **Note:** This notebook uses **CPU machines** since debugging techniques don't require GPU resources.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Debugging distributed training is notoriously difficult:\n",
    "- Issues may only appear on specific ranks or nodes\n",
    "- Traditional debuggers don't work across multiple processes\n",
    "- Environment differences between nodes are hard to inspect\n",
    "- Logs from 128+ processes are overwhelming\n",
    "\n",
    "## Monarch's Solution\n",
    "\n",
    "Monarch provides powerful debugging capabilities:\n",
    "1. **Interactive breakpoints** - Use `pdb` with distributed actors\n",
    "2. **Selective debugging** - Attach to specific ranks\n",
    "3. **Environment inspection** - Query env vars across all nodes\n",
    "4. **Monarch debug CLI** - Unified interface for distributed debugging\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "### Environment Variable Management\n",
    "- Inspect environment variables across nodes\n",
    "- Set and modify env vars remotely\n",
    "- Query variables by prefix (CUDA, NCCL, etc.)\n",
    "\n",
    "### Interactive Debugging with Breakpoints\n",
    "- Add breakpoints to actor methods\n",
    "- Use `monarch debug` CLI\n",
    "- Attach to specific ranks for interactive debugging\n",
    "- Send debugger commands to multiple ranks\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Recommended:** Complete [Studio 1: Getting Started](./studio_1_getting_started.ipynb) and [Studio 2: Workspace Sync](./studio_2_workspace_sync.ipynb) first!\n",
    "\n",
    "You should have:\n",
    "- Basic understanding of Monarch actors and endpoints\n",
    "\n",
    "**New to Monarch?** Start with [Studio 0: Monarch Basics](./studio_0_monarch_basics.ipynb) to learn about Actors, Endpoints, and Process Meshes!\n",
    "\n",
    "## Lightning Studios Series\n",
    "\n",
    "This is **Studio 3** of the series:\n",
    "\n",
    "- **[Studio 0: Monarch Basics](./studio_0_monarch_basics.ipynb)** - Learn Monarch fundamentals\n",
    "- **[Studio 1: Getting Started](./studio_1_getting_started.ipynb)** - Multi-node training (GPU)\n",
    "- **[Studio 2: Workspace Sync](./studio_2_workspace_sync.ipynb)** - Hot-reload configs (CPU)\n",
    "- **Studio 3: Interactive Debugging** - Debug distributed systems (YOU ARE HERE - CPU)\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Retrieve the job or Setup (If Starting Fresh)\n",
    "\n",
    "If you're continuing from Studio 1 or 2, you can retrieve the created job here; otherwise the cell below will create a new job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "# Need to set before importing monarch\n",
    "os.environ[\"MONARCH_FILE_LOG\"] = \"debug\"\n",
    "os.environ[\"HYPERACTOR_MESH_ENABLE_LOG_FORWARDING\"] = \"true\"\n",
    "os.environ[\"HYPERACTOR_MESH_ENABLE_FILE_CAPTURE\"] = \"true\"\n",
    "os.environ[\"HYPERACTOR_MESH_TAIL_LOG_LINES\"] = \"100\"\n",
    "\n",
    "import socket\n",
    "import time\n",
    "\n",
    "from lightning_sdk import Status\n",
    "from utils import get_host_ip_addr, bootstrap_addr\n",
    "from monarch.actor import Actor, enable_transport, endpoint, current_rank\n",
    "from monarch._src.actor.bootstrap import attach_to_workers\n",
    "\n",
    "# Configuration - Using CPU machines for debugging demo\n",
    "NUM_NODES = 2\n",
    "NUM_CPUS = 4  # CPU_X_4 machines have 4 CPUs\n",
    "USE_CPU = True  # Use CPU machines instead of GPU\n",
    "port = 26600\n",
    "\n",
    "# Enable client transport\n",
    "host_ip_addr = get_host_ip_addr(addr_type=\"public\")\n",
    "enable_transport(f\"tcp://{host_ip_addr}:{port}@tcp://0.0.0.0:{port}\")\n",
    "print(f\"Client transport enabled at {host_ip_addr}:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_utils import launch_mmt_job\n",
    "\n",
    "MMT_JOB_NAME = f\"Monarch-v0.2.0-CPU-MMT-{NUM_NODES}-nodes\"\n",
    "\n",
    "# Launch or retrieve the job with CPU machines\n",
    "job, studio = launch_mmt_job(\n",
    "    num_nodes=NUM_NODES,\n",
    "    mmt_job_name=MMT_JOB_NAME,\n",
    "    port=port,\n",
    "    use_cpu=USE_CPU,  # Use CPU machines\n",
    ")\n",
    "\n",
    "print(f\"Job launched. You can monitor it using: job.status\")\n",
    "print(f\"To stop the job: job.stop()\")\n",
    "print(f\"To clean up: studio.stop()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if job.status == Status('Running'):\n",
    "    # Get worker IP addresses from the job\n",
    "    ip_addresses_list_public = [machine.public_ip for machine in job.machines]\n",
    "    print(f\"Worker IPs: {ip_addresses_list_public}\")\n",
    "\n",
    "    # Create worker addresses\n",
    "    worker_addrs = [f\"tcp://{ip}:{port}@tcp://0.0.0.0:{port}\" for ip in ip_addresses_list_public]\n",
    "    print(f\"Worker addresses: {worker_addrs}\")\n",
    "\n",
    "    # Attach to workers and create process mesh\n",
    "    host_mesh = attach_to_workers(\n",
    "        name=\"host_mesh\", ca=\"trust_all_connections\", workers=worker_addrs\n",
    "    )\n",
    "\n",
    "    # Use cpus instead of gpus for CPU machines\n",
    "    proc_mesh = host_mesh.spawn_procs(per_host={\"cpus\": NUM_CPUS})\n",
    "    await proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)\n",
    "\n",
    "    print(f\"\\nProcess mesh initialized successfully!\")\n",
    "    print(f\"Using {NUM_NODES} CPU nodes with {NUM_CPUS} CPUs each\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        f\"Job status is {job.status}; however the status should be {Status('Running')} to initiate the mesh\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Environment Variable Management\n",
    "\n",
    "Let's start by creating an actor that can inspect and manage environment variables across all nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment Variable Actor\n",
    "\n",
    "This actor provides methods to get, set, and list environment variables on remote nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvVarActor(Actor):\n",
    "    \"\"\"Actor for managing environment variables on remote nodes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rank = current_rank().rank\n",
    "        self.hostname = socket.gethostname()\n",
    "\n",
    "    @endpoint\n",
    "    def get_env(self, var_name: str) -> dict:\n",
    "        \"\"\"Get an environment variable value from the remote node.\"\"\"\n",
    "        value = os.environ.get(var_name)\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"var_name\": var_name,\n",
    "            \"value\": value\n",
    "        }\n",
    "\n",
    "    @endpoint\n",
    "    def set_env(self, var_name: str, var_value: str) -> dict:\n",
    "        \"\"\"Set an environment variable on the remote node.\"\"\"\n",
    "        os.environ[var_name] = var_value\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"var_name\": var_name,\n",
    "            \"value\": var_value,\n",
    "            \"status\": \"set\"\n",
    "        }\n",
    "\n",
    "    @endpoint\n",
    "    def list_env_vars(self, prefix: str = \"\") -> dict:\n",
    "        \"\"\"List all environment variables matching a prefix.\"\"\"\n",
    "        matching_vars = {k: v for k, v in os.environ.items() if k.startswith(prefix)}\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"matching_vars\": matching_vars,\n",
    "            \"count\": len(matching_vars)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spawn Environment Variable Actor\n",
    "\n",
    "Spawn the actor across all nodes in the process mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn the environment variable actor across all nodes\n",
    "env_actor = proc_mesh.spawn(\"env_actor\", EnvVarActor)\n",
    "print(\"EnvVarActor spawned across all nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Environment Variables\n",
    "\n",
    "Let's inspect CUDA-related environment variables across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CUDA_VISIBLE_DEVICES from all nodes\n",
    "results = await env_actor.get_env.call(\"CUDA_VISIBLE_DEVICES\")\n",
    "\n",
    "print(\"\\nCUDA_VISIBLE_DEVICES on all nodes:\")\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "# Show unique values by node\n",
    "seen_nodes = set()\n",
    "for result in results:\n",
    "    if len(result) > 1:\n",
    "        rank = result[1].get('rank', '?')\n",
    "        hostname = result[1].get('hostname', '?')\n",
    "        value = result[1].get('value', '?')\n",
    "    else:\n",
    "        rank = result.get('rank', '?')\n",
    "        hostname = result.get('hostname', '?')\n",
    "        value = result.get('value', '?')\n",
    "\n",
    "    if hostname not in seen_nodes:\n",
    "        print(f\"  Node {hostname} (Rank {rank}): {value}\")\n",
    "        seen_nodes.add(hostname)\n",
    "\n",
    "print(f\"{'-'*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Custom Environment Variables\n",
    "\n",
    "You can set environment variables remotely for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a custom environment variable on all nodes\n",
    "print(\"Setting CUSTOM_DEBUG_VAR on all nodes...\")\n",
    "set_results = await env_actor.set_env.call(\"CUSTOM_DEBUG_VAR\", \"debug_enabled\")\n",
    "\n",
    "print(f\"\\nSet CUSTOM_DEBUG_VAR on {len(set_results)} ranks\")\n",
    "\n",
    "# Verify the variable was set\n",
    "verify_results = await env_actor.get_env.call(\"CUSTOM_DEBUG_VAR\")\n",
    "print(f\"\\nVerification (first 3 ranks):\")\n",
    "for i, result in enumerate(verify_results[:3]):\n",
    "    if len(result) > 1:\n",
    "        rank = result[1]['rank']\n",
    "        value = result[1]['value']\n",
    "    else:\n",
    "        rank = result['rank']\n",
    "        value = result['value']\n",
    "    print(f\"  Rank {rank}: CUSTOM_DEBUG_VAR = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Variables by Prefix\n",
    "\n",
    "Query all environment variables matching a specific prefix - useful for debugging CUDA, NCCL, or PyTorch settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all CUDA-related environment variables\n",
    "list_results = await env_actor.list_env_vars.call(\"CUDA\")\n",
    "\n",
    "print(\"\\nCUDA-related environment variables (Rank 0):\")\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "if list_results[0]:\n",
    "    result = list_results[0][1] if len(list_results[0]) > 1 else list_results[0]\n",
    "    matching_vars = result.get('matching_vars', {})\n",
    "\n",
    "    if matching_vars:\n",
    "        for var_name, var_value in matching_vars.items():\n",
    "            # Truncate long values\n",
    "            display_value = var_value if len(var_value) < 60 else var_value[:57] + \"...\"\n",
    "            print(f\"  {var_name} = {display_value}\")\n",
    "    else:\n",
    "        print(\"  No CUDA variables found\")\n",
    "\n",
    "print(f\"{'-'*70}\")\n",
    "\n",
    "# Try other prefixes\n",
    "print(\"\\nTip: Try querying other prefixes like:\")\n",
    "print(\"  - 'NCCL' - NCCL communication settings\")\n",
    "print(\"  - 'TORCH' - PyTorch settings\")\n",
    "print(\"  - 'MONARCH' - Monarch-specific configs\")\n",
    "print(\"  - 'MASTER' - Distributed training master node info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Interactive Debugging with Breakpoints\n",
    "\n",
    "Now let's explore Monarch's most powerful debugging feature: **interactive breakpoints** in distributed actors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Monarch Debugging Works\n",
    "\n",
    "### The Workflow\n",
    "\n",
    "1. **Add `breakpoint()`** to your actor methods\n",
    "2. **Run your code** - execution pauses when breakpoint is hit\n",
    "3. **Open a terminal** and run `monarch debug`\n",
    "4. **Use debugger commands**:\n",
    "   - `list` - Show all active breakpoints\n",
    "   - `attach <actor> <rank>` - Attach to a specific rank\n",
    "   - Standard pdb commands: `n`, `s`, `p`, `l`, `c`\n",
    "   - `cast <actor> ranks(<ranks>) <cmd>` - Send commands to multiple ranks\n",
    "   - `continue` - Resume all paused processes\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- Debug specific ranks (e.g., only rank 0 or only GPU 3)\n",
    "- Inspect local variables and actor state\n",
    "- Step through code interactively\n",
    "- Send commands to multiple ranks simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Debug Worker Actor\n",
    "\n",
    "Let's create a simple worker actor with strategic breakpoints. This actor simulates a distributed computation workflow without requiring GPU or TorchTitan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "class DebugWorkerActor(Actor):\n",
    "    \"\"\"A simple worker actor with debugging breakpoints for CPU-based debugging demo.\"\"\"\n",
    "\n",
    "    def __init__(self, worker_name: str = \"worker\"):\n",
    "        self.rank = current_rank().rank\n",
    "        self.worker_name = worker_name\n",
    "        self.hostname = socket.gethostname()\n",
    "        self.step_count = 0\n",
    "        self.data = []\n",
    "\n",
    "    def _rprint(self, msg):\n",
    "        \"\"\"Helper method to print with rank information.\"\"\"\n",
    "        print(f\"[Rank {self.rank}] {msg}\")\n",
    "\n",
    "    @endpoint\n",
    "    def init(self):\n",
    "        \"\"\"Initialize the worker with a breakpoint.\"\"\"\n",
    "        logging.getLogger().addHandler(logging.StreamHandler(sys.stderr))\n",
    "        self._rprint(f\"Initializing worker: {self.worker_name} on {self.hostname}\")\n",
    "\n",
    "        # Breakpoint 1: After initialization (only on rank 0)\n",
    "        if self.rank == 0:\n",
    "            self._rprint(\"Breakpoint 1: Initialization complete\")\n",
    "            breakpoint()  # Debug: Inspect actor initialization state\n",
    "\n",
    "    @endpoint\n",
    "    def setup(self, data_size: int = 100):\n",
    "        \"\"\"Setup the worker with some data, with a breakpoint to inspect configuration.\"\"\"\n",
    "        self._rprint(f\"Setting up worker with data_size={data_size}\")\n",
    "\n",
    "        # Generate some random data for this worker\n",
    "        self.data = [random.random() for _ in range(data_size)]\n",
    "\n",
    "        # Breakpoint 2: After data setup (only on rank 0)\n",
    "        if self.rank == 0:\n",
    "            self._rprint(f\"Breakpoint 2: Data setup complete, data length={len(self.data)}\")\n",
    "            breakpoint()  # Debug: Inspect data after setup\n",
    "\n",
    "        self._rprint(f\"Setup complete with {len(self.data)} data points\")\n",
    "\n",
    "    @endpoint\n",
    "    def process_step(self, num_steps: int = 5):\n",
    "        \"\"\"Run a few processing steps with breakpoints.\"\"\"\n",
    "        if not self.data:\n",
    "            raise RuntimeError(\"Worker not initialized. Call setup first.\")\n",
    "\n",
    "        self._rprint(f\"Starting processing for {num_steps} steps\")\n",
    "\n",
    "        # Breakpoint 3: Before processing starts (only on rank 0)\n",
    "        if self.rank == 0:\n",
    "            self._rprint(\"Breakpoint 3: About to start processing\")\n",
    "            breakpoint()  # Debug: Inspect state before processing\n",
    "\n",
    "        results = []\n",
    "        for step in range(num_steps):\n",
    "            self.step_count += 1\n",
    "\n",
    "            # Simulate some computation\n",
    "            step_result = sum(self.data) / len(self.data)\n",
    "            results.append(step_result)\n",
    "\n",
    "            # Breakpoint 4: Mid-processing on rank 0 at step 2\n",
    "            if step == 2 and self.rank == 0:\n",
    "                self._rprint(f\"Breakpoint 4: Mid-processing (step {self.step_count})\")\n",
    "                self._rprint(f\"Current result: {step_result:.4f}\")\n",
    "                breakpoint()  # Debug: Inspect mid-processing state\n",
    "\n",
    "            self._rprint(f\"Processing step {step + 1}/{num_steps}, result: {step_result:.4f}\")\n",
    "\n",
    "        self._rprint(f\"Completed {num_steps} processing steps\")\n",
    "        return {\"rank\": self.rank, \"steps\": num_steps, \"final_result\": results[-1]}\n",
    "\n",
    "    @endpoint\n",
    "    def get_status(self) -> dict:\n",
    "        \"\"\"Get current worker status.\"\"\"\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"worker_name\": self.worker_name,\n",
    "            \"step_count\": self.step_count,\n",
    "            \"data_size\": len(self.data)\n",
    "        }\n",
    "\n",
    "    @endpoint\n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup resources.\"\"\"\n",
    "        self._rprint(\"Cleaning up worker\")\n",
    "        self.data = []\n",
    "        self.step_count = 0\n",
    "        self._rprint(\"Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spawn Debug Worker\n",
    "\n",
    "Spawn the debug worker actor. When you run the cells below, execution will pause at breakpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn the debug worker actor\n",
    "debug_worker = proc_mesh.spawn(\"debug_worker\", DebugWorkerActor, \"demo_worker\")\n",
    "print(\"Debug worker actor spawned across all nodes\")\n",
    "print(\"\\nWhen breakpoints are hit, execution will pause.\")\n",
    "print(\"Open a separate terminal and run: monarch debug\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Debug Session\n",
    "\n",
    "Now let's run the training methods. When breakpoints are hit:\n",
    "\n",
    "### In This Notebook\n",
    "- Execution will pause\n",
    "- You'll see `Breakpoint X: ...` messages\n",
    "\n",
    "### In a Separate Terminal\n",
    "1. Run: `monarch debug`\n",
    "2. Use `list` to see all active breakpoints\n",
    "3. Use `attach debug_trainer 0` to attach to rank 0\n",
    "4. Use standard pdb commands or `continue` to resume\n",
    "\n",
    "**Note:** For this demo, we'll skip the interactive debugging. In practice, you'd have two terminals open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize workers (will hit breakpoint 1)\n",
    "print(\"Step 1: Initializing workers...\")\n",
    "print(\"   (Breakpoint 1 will trigger on rank 0)\\n\")\n",
    "\n",
    "# In a real scenario, this would pause at the breakpoint\n",
    "# await debug_worker.init.call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup workers with data (will hit breakpoint 2)\n",
    "print(\"Step 2: Setting up workers with data...\")\n",
    "print(\"   (Breakpoint 2 will trigger on rank 0)\\n\")\n",
    "\n",
    "# await debug_worker.setup.call(data_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run processing steps (will hit breakpoints 3 and 4)\n",
    "print(\"Step 3: Running processing steps...\")\n",
    "print(\"   (Breakpoints 3 and 4 will trigger on rank 0)\\n\")\n",
    "\n",
    "# await debug_worker.process_step.call(num_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monarch Debug CLI Commands\n",
    "\n",
    "Here's a quick reference for the `monarch debug` CLI:\n",
    "\n",
    "### Listing Breakpoints\n",
    "```bash\n",
    "monarch_dbg> list\n",
    "# Shows all active breakpoints across ranks\n",
    "# Example output:\n",
    "#   debug_trainer (rank 0): /path/to/file.py:42\n",
    "#   debug_trainer (rank 0): /path/to/file.py:58\n",
    "```\n",
    "\n",
    "### Attaching to a Rank\n",
    "```bash\n",
    "monarch_dbg> attach debug_trainer 0\n",
    "# Enters interactive pdb session for rank 0\n",
    "\n",
    "(Pdb) n              # Next line\n",
    "(Pdb) s              # Step into function\n",
    "(Pdb) p self.rank    # Print variable\n",
    "(Pdb) l              # List source code\n",
    "(Pdb) pp self.job_config  # Pretty-print object\n",
    "(Pdb) c              # Continue execution\n",
    "```\n",
    "\n",
    "### Casting Commands to Multiple Ranks\n",
    "```bash\n",
    "# Send \"next\" command to ranks 0 and 1\n",
    "monarch_dbg> cast debug_trainer ranks(0,1) n\n",
    "\n",
    "# Send \"continue\" to ranks 0 through 7\n",
    "monarch_dbg> cast debug_trainer ranks(0:8) c\n",
    "\n",
    "# Print a variable on multiple ranks\n",
    "monarch_dbg> cast debug_trainer ranks(0,1,2,3) p self.step_count\n",
    "```\n",
    "\n",
    "### Continuing All\n",
    "```bash\n",
    "monarch_dbg> continue\n",
    "# Resumes execution on all paused ranks\n",
    "```\n",
    "\n",
    "### Getting Help\n",
    "```bash\n",
    "monarch_dbg> help\n",
    "# Shows all available commands\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Debugging Scenarios\n",
    "\n",
    "### Scenario 1: Rank-Specific Bug\n",
    "```python\n",
    "# Problem: Training fails on rank 5 but works on other ranks\n",
    "\n",
    "@endpoint\n",
    "def train(self):\n",
    "    if self.rank == 5:\n",
    "        breakpoint()  # Only pause rank 5\n",
    "    # ... training code\n",
    "```\n",
    "\n",
    "Then in terminal:\n",
    "```bash\n",
    "monarch debug\n",
    "monarch_dbg> attach trainer_actor 5\n",
    "(Pdb) p self.data_batch  # Inspect what's different on rank 5\n",
    "```\n",
    "\n",
    "### Scenario 2: Collective Operation Hang\n",
    "```python\n",
    "# Problem: All-reduce hangs, need to check all ranks\n",
    "\n",
    "@endpoint\n",
    "def sync_gradients(self):\n",
    "    breakpoint()  # Pause all ranks before all-reduce\n",
    "    torch.distributed.all_reduce(self.gradients)\n",
    "```\n",
    "\n",
    "Then:\n",
    "```bash\n",
    "monarch_dbg> list  # Check which ranks hit the breakpoint\n",
    "monarch_dbg> cast trainer_actor ranks(0:8) p self.gradients.shape\n",
    "# Verify all ranks have same shape\n",
    "```\n",
    "\n",
    "### Scenario 3: Environment Mismatch\n",
    "```python\n",
    "# Problem: Different NCCL settings causing issues\n",
    "\n",
    "# Use EnvVarActor to inspect\n",
    "results = await env_actor.list_env_vars.call(\"NCCL\")\n",
    "# Compare NCCL settings across all ranks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Congratulations!\n",
    "\n",
    "You've mastered **interactive debugging** for distributed training with Monarch!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "### Environment Variable Management\n",
    "- Query env vars across all nodes\n",
    "- Set and modify env vars remotely\n",
    "- List variables by prefix (CUDA, NCCL, etc.)\n",
    "\n",
    "### Interactive Debugging\n",
    "- Add breakpoints to distributed actors\n",
    "- Use `monarch debug` CLI\n",
    "- Attach to specific ranks\n",
    "- Send commands to multiple ranks\n",
    "- Common debugging scenarios\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Debug like local code** - Use familiar pdb commands in distributed settings\n",
    "- **Selective debugging** - Focus on problematic ranks without noise from others\n",
    "- **Environment inspection** - Quickly identify configuration mismatches\n",
    "- **No more print debugging** - Interactive inspection is much more powerful\n",
    "\n",
    "## The Complete Monarch Workflow\n",
    "\n",
    "You've now learned the three pillars of efficient distributed development:\n",
    "\n",
    "1. **Studio 1: Getting Started** - Launch multi-node training\n",
    "2. **Studio 2: Workspace Sync** - Hot-reload configs and code\n",
    "3. **Studio 3: Interactive Debugging** - Debug efficiently (YOU ARE HERE!)\n",
    "\n",
    "Together, these enable:\n",
    "- **10x faster iteration** (no job restarts)\n",
    "- **Easier debugging** (interactive breakpoints)\n",
    "- **Better observability** (env var inspection, log aggregation)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Put It Into Practice\n",
    "Try debugging your own training code:\n",
    "1. Add strategic breakpoints\n",
    "2. Run `monarch debug` when they're hit\n",
    "3. Inspect state and identify issues\n",
    "\n",
    "### Explore More\n",
    "- Review [Studio 1: Getting Started](./studio_1_getting_started.ipynb)\n",
    "- Review [Studio 2: Workspace Sync](./studio_2_workspace_sync.ipynb)\n",
    "- Check out the [Monarch documentation](https://github.com/meta-pytorch/monarch)\n",
    "\n",
    "---\n",
    "\n",
    "## Pro Tips\n",
    "\n",
    "### Debugging Best Practices\n",
    "1. **Use conditional breakpoints** - Only pause specific ranks\n",
    "2. **Check env vars first** - Many issues are configuration mismatches\n",
    "3. **Use `cast` for comparison** - Check variables across multiple ranks\n",
    "4. **Don't forget `continue`** - Resume execution when done debugging\n",
    "\n",
    "### Performance Tip\n",
    "Remove or comment out `breakpoint()` calls for production runs - they have minimal overhead when not triggered, but it's cleaner to remove them.\n",
    "\n",
    "Happy debugging!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
