{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studio 2: Hot-Reloading with Workspace Synchronization\n",
    "\n",
    "Welcome to Studio 2! In this notebook, you'll learn one of Monarch's most powerful features: **workspace synchronization**.\n",
    "\n",
    "> **Note:** This notebook uses **CPU machines** since workspace synchronization doesn't require GPU resources.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "In traditional distributed training:\n",
    "1. You launch a multi-node job (takes 5-10 minutes)\n",
    "2. You realize you need to change a config value (e.g., learning rate)\n",
    "3. You have to **stop everything** and restart (another 5-10 minutes)\n",
    "4. Rinse and repeat...\n",
    "\n",
    "This is incredibly frustrating and wastes valuable time and compute resources!\n",
    "\n",
    "## The Solution: Workspace Sync\n",
    "\n",
    "With Monarch's `proc_mesh.sync_workspace()`:\n",
    "1. Launch your multi-node job once\n",
    "2. Edit configs or code **locally**\n",
    "3. Run `sync_workspace()` to propagate changes to all remote nodes\n",
    "4. Re-run training with updated configs - **no restart needed!**\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How workspace synchronization works\n",
    "- Creating and modifying training configs locally\n",
    "- Syncing changes to remote worker nodes\n",
    "- Verifying synchronization across the cluster\n",
    "- Practical hot-reload workflows\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Required:** Complete [Studio 1: Getting Started](./studio_1_getting_started.ipynb) first!\n",
    "\n",
    "You should have:\n",
    "- Basic understanding of Monarch actors\n",
    "- Familiarity with launching MMT jobs\n",
    "\n",
    "**New to Monarch?** Start with [Studio 0: Monarch Basics](./studio_0_monarch_basics.ipynb) to learn the fundamentals!\n",
    "\n",
    "## Lightning Studios Series\n",
    "\n",
    "This is **Studio 2** of the series:\n",
    "\n",
    "- **[Studio 0: Monarch Basics](./studio_0_monarch_basics.ipynb)** - Learn Monarch fundamentals\n",
    "- **[Studio 1: Getting Started](./studio_1_getting_started.ipynb)** - Multi-node training (GPU)\n",
    "- **Studio 2: Workspace Sync** - Hot-reload configs (YOU ARE HERE - CPU)\n",
    "- **[Studio 3: Interactive Debugging](./studio_3_interactive_debugging.ipynb)** - Debug distributed systems (CPU)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Retrieve the job or Setup (If Starting Fresh)\n",
    "\n",
    "If you're continuing from Studio 1, you can retrieve the created job here; otherwise the cell below will create a new job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Need to set before importing monarch\n",
    "os.environ[\"MONARCH_FILE_LOG\"] = \"debug\"\n",
    "os.environ[\"HYPERACTOR_MESH_ENABLE_LOG_FORWARDING\"] = \"true\"\n",
    "os.environ[\"HYPERACTOR_MESH_ENABLE_FILE_CAPTURE\"] = \"true\"\n",
    "os.environ[\"HYPERACTOR_MESH_TAIL_LOG_LINES\"] = \"100\"\n",
    "\n",
    "import socket\n",
    "import time\n",
    "\n",
    "from lightning_sdk import Status\n",
    "from utils import get_host_ip_addr, bootstrap_addr\n",
    "from monarch.actor import Actor, enable_transport, endpoint, current_rank\n",
    "from monarch._src.actor.bootstrap import attach_to_workers\n",
    "\n",
    "# Configuration - Using CPU machines for workspace sync demo\n",
    "NUM_NODES = 2\n",
    "NUM_CPUS = 4  # CPU_X_4 machines have 4 CPUs\n",
    "USE_CPU = True  # Use CPU machines instead of GPU\n",
    "port = 26600\n",
    "\n",
    "# Enable client transport\n",
    "host_ip_addr = get_host_ip_addr(addr_type=\"public\")\n",
    "enable_transport(f\"tcp://{host_ip_addr}:{port}@tcp://0.0.0.0:{port}\")\n",
    "print(f\"Client transport enabled at {host_ip_addr}:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmt_utils import launch_mmt_job\n",
    "\n",
    "MMT_JOB_NAME = f\"Monarch-v0.2.0-MMT-{NUM_NODES}-nodes\"\n",
    "\n",
    "# Launch or retrieve the job with CPU machines\n",
    "job, studio = launch_mmt_job(\n",
    "    num_nodes=NUM_NODES,\n",
    "    mmt_job_name=MMT_JOB_NAME,\n",
    "    port=port,\n",
    "    use_cpu=USE_CPU,  # Use CPU machines\n",
    ")\n",
    "\n",
    "print(f\"Job launched. You can monitor it using: job.status\")\n",
    "print(f\"To stop the job: job.stop()\")\n",
    "print(f\"To clean up: studio.stop()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if job.status == Status('Running'):\n",
    "    # Get worker IP addresses from the job\n",
    "    ip_addresses_list_public = [machine.public_ip for machine in job.machines]\n",
    "    print(f\"Worker IPs: {ip_addresses_list_public}\")\n",
    "\n",
    "    # Create worker addresses\n",
    "    worker_addrs = [f\"tcp://{ip}:{port}@tcp://0.0.0.0:{port}\" for ip in ip_addresses_list_public]\n",
    "    print(f\"Worker addresses: {worker_addrs}\")\n",
    "\n",
    "    # Attach to workers and create process mesh\n",
    "    host_mesh = attach_to_workers(\n",
    "        name=\"host_mesh\", ca=\"trust_all_connections\", workers=worker_addrs\n",
    "    )\n",
    "\n",
    "    # Use cpus instead of gpus for CPU machines\n",
    "    proc_mesh = host_mesh.spawn_procs(per_host={\"cpus\": NUM_CPUS})\n",
    "    await proc_mesh.logging_option(stream_to_client=True, aggregate_window_sec=3)\n",
    "\n",
    "    print(f\"\\nProcess mesh initialized successfully!\")\n",
    "    print(f\"Using {NUM_NODES} CPU nodes with {NUM_CPUS} CPUs each\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        f\"Job status is {job.status}; however the status should be {Status('Running')} to initiate the mesh\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Workspace Synchronization Workflow\n",
    "\n",
    "Let's dive into workspace sync with a practical example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define File Checker Actor\n",
    "\n",
    "First, we'll create an actor that can read and verify file contents on remote nodes. This helps us confirm that files are properly synchronized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileCheckerActor(Actor):\n",
    "    \"\"\"Actor to read and verify file contents on remote nodes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rank = current_rank().rank\n",
    "        self.hostname = socket.gethostname()\n",
    "\n",
    "    @endpoint\n",
    "    def read_file(self, file_path: str) -> dict:\n",
    "        \"\"\"Read a file and return its contents.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            return {\n",
    "                \"rank\": self.rank,\n",
    "                \"hostname\": self.hostname,\n",
    "                \"file_path\": file_path,\n",
    "                \"content\": content,\n",
    "                \"exists\": True,\n",
    "                \"size\": len(content)\n",
    "            }\n",
    "        except FileNotFoundError:\n",
    "            return {\n",
    "                \"rank\": self.rank,\n",
    "                \"hostname\": self.hostname,\n",
    "                \"file_path\": file_path,\n",
    "                \"exists\": False,\n",
    "                \"error\": \"File not found\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"rank\": self.rank,\n",
    "                \"hostname\": self.hostname,\n",
    "                \"file_path\": file_path,\n",
    "                \"exists\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    @endpoint\n",
    "    def file_exists(self, file_path: str) -> dict:\n",
    "        \"\"\"Check if a file exists on the remote node.\"\"\"\n",
    "        exists = os.path.exists(file_path)\n",
    "        return {\n",
    "            \"rank\": self.rank,\n",
    "            \"hostname\": self.hostname,\n",
    "            \"file_path\": file_path,\n",
    "            \"exists\": exists\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spawn File Checker Actor\n",
    "\n",
    "Spawn the file checker actor across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn the file checker actor\n",
    "file_checker = proc_mesh.spawn(\"file_checker\", FileCheckerActor)\n",
    "print(\"FileCheckerActor spawned across all nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Local Configuration File\n",
    "\n",
    "Let's create a training configuration file locally. This simulates a common workflow where you want to tweak hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local workspace directory for our custom config\n",
    "local_workspace = \"/teamspace/studios/this_studio/monarch_sync_example\"\n",
    "os.makedirs(local_workspace, exist_ok=True)\n",
    "\n",
    "# Create a custom training configuration file\n",
    "config_file_name = \"custom_training_config.toml\"\n",
    "local_config_path = os.path.join(local_workspace, config_file_name)\n",
    "\n",
    "# Write initial configuration\n",
    "initial_config = \"\"\"# TorchTitan Custom Training Configuration\n",
    "# Version 1.0 - Initial configuration\n",
    "\n",
    "[training]\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "max_steps = 100\n",
    "warmup_steps = 10\n",
    "\n",
    "[model]\n",
    "model_type = \"llama3_8b\"\n",
    "seq_len = 1024\n",
    "\n",
    "[optimizer]\n",
    "optimizer_type = \"AdamW\"\n",
    "weight_decay = 0.01\n",
    "\"\"\"\n",
    "\n",
    "with open(local_config_path, 'w') as f:\n",
    "    f.write(initial_config)\n",
    "\n",
    "print(f\"Created local config file: {local_config_path}\")\n",
    "print(f\"\\nInitial configuration:\\n{'-'*50}\")\n",
    "print(initial_config)\n",
    "print(f\"{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Workspace and Perform Initial Sync\n",
    "\n",
    "Now we'll create a Monarch `Workspace` object and sync our local directory to all remote nodes.\n",
    "\n",
    "**This is the magic step!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from monarch.tools.config.workspace import Workspace\nfrom pathlib import Path\n\n# Create a Workspace object pointing to our local directory\nworkspace = Workspace(dirs=[Path(local_workspace)])\n\nprint(f\"Workspace configured: {workspace.dirs}\")\nprint(f\"\\nSyncing workspace to {NUM_NODES * NUM_CPUS} remote processes...\")\n\n# Perform initial sync\nawait proc_mesh.sync_workspace(workspace=workspace, conda=False, auto_reload=False)\n\nprint(\"\\nInitial workspace sync completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify File on Remote Nodes\n",
    "\n",
    "Let's verify that our config file was successfully synced to all remote worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the remote file path (files are synced to WORKSPACE_DIR)\n",
    "remote_workspace_root = os.environ.get(\"WORKSPACE_DIR\", \"/workspace\")\n",
    "remote_config_path = os.path.join(remote_workspace_root, \"monarch_sync_example\", config_file_name)\n",
    "\n",
    "print(f\"Checking file on remote nodes: {remote_config_path}\\n\")\n",
    "\n",
    "# Check file existence on all nodes (just check first rank of each node)\n",
    "exists_results = await file_checker.file_exists.call(remote_config_path)\n",
    "\n",
    "# Group by hostname to show node-level status\n",
    "nodes_checked = set()\n",
    "for result in exists_results:\n",
    "    hostname = result['hostname']\n",
    "    if hostname not in nodes_checked:\n",
    "        status = \"EXISTS\" if result['exists'] else \"NOT FOUND\"\n",
    "        print(f\"  Node {hostname}: {status}\")\n",
    "        nodes_checked.add(hostname)\n",
    "\n",
    "# Read file content from rank 0 to verify\n",
    "print(f\"\\nReading config from rank 0:\")\n",
    "print(f\"{'-'*50}\")\n",
    "read_results = await file_checker.read_file.call(remote_config_path)\n",
    "if read_results[0]['exists']:\n",
    "    print(read_results[0]['content'])\n",
    "else:\n",
    "    print(f\"Error: {read_results[0].get('error', 'Unknown error')}\")\n",
    "print(f\"{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Hot-Reload: Modify and Re-Sync\n",
    "\n",
    "Now comes the powerful part! Let's modify our config locally and sync it again - **without restarting anything**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Local Configuration\n",
    "\n",
    "Let's say we want to:\n",
    "- Decrease the learning rate (0.001 -> 0.0005)\n",
    "- Increase max steps (100 -> 200)\n",
    "- Change sequence length (1024 -> 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the configuration\n",
    "updated_config = \"\"\"# TorchTitan Custom Training Configuration\n",
    "# Version 2.0 - Updated after initial run\n",
    "\n",
    "[training]\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005  # <- CHANGED: Reduced from 0.001\n",
    "max_steps = 200          # <- CHANGED: Increased from 100\n",
    "warmup_steps = 10\n",
    "\n",
    "[model]\n",
    "model_type = \"llama3_8b\"\n",
    "seq_len = 2048           # <- CHANGED: Increased from 1024\n",
    "\n",
    "[optimizer]\n",
    "optimizer_type = \"AdamW\"\n",
    "weight_decay = 0.01\n",
    "\"\"\"\n",
    "\n",
    "# Write updated config locally\n",
    "with open(local_config_path, 'w') as f:\n",
    "    f.write(updated_config)\n",
    "\n",
    "print(f\"Updated local config file: {local_config_path}\")\n",
    "print(f\"\\nUpdated configuration:\\n{'-'*50}\")\n",
    "print(updated_config)\n",
    "print(f\"{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Sync to Remote Nodes\n",
    "\n",
    "Now sync the changes to all remote nodes. This is instant - no job restart required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Re-syncing updated workspace to remote nodes...\")\n",
    "\n",
    "# Sync again - Monarch only transfers what changed!\n",
    "await proc_mesh.sync_workspace(workspace=workspace, conda=False, auto_reload=False)\n",
    "\n",
    "print(\"\\nWorkspace re-sync completed!\")\n",
    "print(\"\\nThe updated config is now available on all remote nodes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Updated File on Remote Nodes\n",
    "\n",
    "Let's confirm the updated config made it to the remote nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reading updated config from rank 0:\")\n",
    "print(f\"{'-'*50}\")\n",
    "\n",
    "read_results = await file_checker.read_file.call(remote_config_path)\n",
    "if read_results[0]['exists']:\n",
    "    remote_content = read_results[0]['content']\n",
    "    print(remote_content)\n",
    "\n",
    "    # Verify it matches our local update\n",
    "    if \"learning_rate = 0.0005\" in remote_content and \"max_steps = 200\" in remote_content:\n",
    "        print(f\"{'-'*50}\")\n",
    "        print(\"\\nSUCCESS! Remote config matches local changes:\")\n",
    "        print(\"  Learning rate: 0.001 -> 0.0005\")\n",
    "        print(\"  Max steps: 100 -> 200\")\n",
    "        print(\"  Sequence length: 1024 -> 2048\")\n",
    "    else:\n",
    "        print(f\"{'-'*50}\")\n",
    "        print(\"\\nWarning: Remote config may not have updated correctly\")\n",
    "else:\n",
    "    print(f\"Error: {read_results[0].get('error', 'Unknown error')}\")\n",
    "    print(f\"{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Real-World Workflow Example\n",
    "\n",
    "Here's how you'd use workspace sync in a real training scenario:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: Iterative Training with Config Changes\n",
    "\n",
    "```python\n",
    "# 1. Initial training run\n",
    "await async_main(config)  # Train with initial settings\n",
    "\n",
    "# 2. Review results, decide to adjust learning rate\n",
    "# Edit local config file...\n",
    "\n",
    "# 3. Sync changes (< 1 second)\n",
    "await proc_mesh.sync_workspace(workspace=workspace)\n",
    "\n",
    "# 4. Re-run training with new config (no restart!)\n",
    "config = config_manager.parse_args(manual_args)  # Reload config\n",
    "await async_main(config)  # Train with updated settings\n",
    "\n",
    "# 5. Repeat as needed!\n",
    "```\n",
    "\n",
    "### Time Savings\n",
    "\n",
    "**Without Monarch:**\n",
    "- Change config: 1 min\n",
    "- Stop job: 1 min\n",
    "- Restart job: 5-10 min\n",
    "- **Total per iteration: ~7-12 min**\n",
    "\n",
    "**With Monarch:**\n",
    "- Change config: 1 min\n",
    "- Sync: < 1 sec\n",
    "- **Total per iteration: ~1 min**\n",
    "\n",
    "**10x faster iteration!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Syncing Multiple Files and Directories\n",
    "\n",
    "You can sync entire directory trees, not just single files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Sync multiple directories\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a workspace with multiple directories\n",
    "multi_dir_workspace = Workspace(dirs=[\n",
    "    Path(\"/teamspace/studios/this_studio/configs\"),\n",
    "    Path(\"/teamspace/studios/this_studio/custom_modules\"),\n",
    "    Path(\"/teamspace/studios/this_studio/data_processors\"),\n",
    "])\n",
    "\n",
    "# Sync all directories at once\n",
    "# await proc_mesh.sync_workspace(workspace=multi_dir_workspace)\n",
    "\n",
    "print(\"\\nTip: You can sync entire project directories, not just config files!\")\n",
    "print(\"This enables hot-reloading of:\")\n",
    "print(\"  - Training scripts\")\n",
    "print(\"  - Model definitions\")\n",
    "print(\"  - Data preprocessing code\")\n",
    "print(\"  - Custom layers and modules\")\n",
    "print(\"  - And more!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Congratulations!\n",
    "\n",
    "You've mastered **workspace synchronization** with Monarch!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "- Creating a Monarch `Workspace` for local directories\n",
    "- Syncing files to remote nodes with `proc_mesh.sync_workspace()`\n",
    "- Verifying synchronization across the cluster\n",
    "- Hot-reloading configs without job restarts\n",
    "- Real-world iterative training workflows\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **10x faster iteration** - No more waiting for job restarts\n",
    "- **Edit locally, run remotely** - Keep your familiar dev environment\n",
    "- **Sync is smart** - Only changed files are transferred\n",
    "- **Works with any files** - Configs, code, data processors, etc.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Studio 3: Interactive Debugging (Recommended Next)\n",
    "Learn advanced debugging techniques:\n",
    "- Set breakpoints in distributed actors\n",
    "- Debug specific ranks with `monarch debug`\n",
    "- Inspect and modify environment variables\n",
    "- Troubleshoot training issues interactively\n",
    "\n",
    "### Back to Studio 1\n",
    "Review the basics: [Studio 1: Getting Started](./studio_1_getting_started.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Try It Yourself!\n",
    "\n",
    "Before moving on, try modifying the config one more time:\n",
    "1. Change the batch size to 64\n",
    "2. Sync the workspace\n",
    "3. Verify the changes\n",
    "\n",
    "This workflow will become second nature!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}